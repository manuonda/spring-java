* 
* ==> Audit <==
* |-----------|----------------------------------|----------|----------|---------|---------------------|---------------------|
|  Command  |               Args               | Profile  |   User   | Version |     Start Time      |      End Time       |
|-----------|----------------------------------|----------|----------|---------|---------------------|---------------------|
| service   | msv-usuarios --url               | minikube | manuonda | v1.32.0 | 22 Jan 24 23:18 -03 |                     |
| service   | msvc-usuarios --url              | minikube | manuonda | v1.32.0 | 22 Jan 24 23:18 -03 | 22 Jan 24 23:18 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 23 Jan 24 22:59 -03 | 23 Jan 24 22:59 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 24 Jan 24 23:00 -03 | 24 Jan 24 23:01 -03 |
| dashboard |                                  | minikube | manuonda | v1.32.0 | 24 Jan 24 23:29 -03 |                     |
| service   | msvc-curso --url                 | minikube | manuonda | v1.32.0 | 25 Jan 24 00:23 -03 |                     |
| service   | msvc-cursos --url                | minikube | manuonda | v1.32.0 | 25 Jan 24 00:24 -03 |                     |
| service   | msvc-curso --url                 | minikube | manuonda | v1.32.0 | 25 Jan 24 00:24 -03 |                     |
| service   | msvc-curso                       | minikube | manuonda | v1.32.0 | 25 Jan 24 00:24 -03 |                     |
| service   | msvc-usuario                     | minikube | manuonda | v1.32.0 | 25 Jan 24 00:25 -03 |                     |
| service   | msvc-usuarios                    | minikube | manuonda | v1.32.0 | 25 Jan 24 00:25 -03 | 25 Jan 24 00:25 -03 |
| service   | msvc-curso                       | minikube | manuonda | v1.32.0 | 25 Jan 24 00:25 -03 |                     |
| service   | msvc-curso                       | minikube | manuonda | v1.32.0 | 25 Jan 24 00:27 -03 |                     |
| service   | msvc-curso                       | minikube | manuonda | v1.32.0 | 25 Jan 24 00:31 -03 |                     |
| service   | pod/msvc-cursos-68cf58fcd9-lsjqt | minikube | manuonda | v1.32.0 | 25 Jan 24 00:33 -03 |                     |
| service   | list                             | minikube | manuonda | v1.32.0 | 25 Jan 24 00:33 -03 | 25 Jan 24 00:33 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 25 Jan 24 08:09 -03 | 25 Jan 24 08:09 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 25 Jan 24 13:51 -03 | 25 Jan 24 13:51 -03 |
| service   |                                  | minikube | manuonda | v1.32.0 | 25 Jan 24 16:48 -03 |                     |
| service   | msvc-curso --url                 | minikube | manuonda | v1.32.0 | 25 Jan 24 16:48 -03 |                     |
| service   | all                              | minikube | manuonda | v1.32.0 | 25 Jan 24 23:09 -03 |                     |
| service   |                                  | minikube | manuonda | v1.32.0 | 25 Jan 24 23:09 -03 |                     |
| service   | --all                            | minikube | manuonda | v1.32.0 | 25 Jan 24 23:09 -03 |                     |
| service   |                                  | minikube | manuonda | v1.32.0 | 25 Jan 24 23:42 -03 |                     |
| service   | a                                | minikube | manuonda | v1.32.0 | 25 Jan 24 23:42 -03 |                     |
| service   | msvc-usuarios                    | minikube | manuonda | v1.32.0 | 25 Jan 24 23:42 -03 | 25 Jan 24 23:42 -03 |
| service   | msvc-cursos                      | minikube | manuonda | v1.32.0 | 25 Jan 24 23:42 -03 |                     |
| service   | msvc-curso                       | minikube | manuonda | v1.32.0 | 25 Jan 24 23:43 -03 |                     |
| service   | msvc-curso                       | minikube | manuonda | v1.32.0 | 25 Jan 24 23:47 -03 |                     |
| service   |                                  | minikube | manuonda | v1.32.0 | 25 Jan 24 23:50 -03 |                     |
| service   | --all                            | minikube | manuonda | v1.32.0 | 25 Jan 24 23:50 -03 |                     |
| start     |                                  | minikube | manuonda | v1.32.0 | 26 Jan 24 09:23 -03 | 26 Jan 24 09:23 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 28 Jan 24 12:55 -03 | 28 Jan 24 12:55 -03 |
| service   |                                  | minikube | manuonda | v1.32.0 | 28 Jan 24 20:00 -03 |                     |
| service   | --all                            | minikube | manuonda | v1.32.0 | 28 Jan 24 20:00 -03 |                     |
| service   | --all                            | minikube | manuonda | v1.32.0 | 28 Jan 24 21:46 -03 |                     |
| start     |                                  | minikube | manuonda | v1.32.0 | 30 Jan 24 22:01 -03 | 30 Jan 24 22:02 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 30 Jan 24 23:41 -03 | 30 Jan 24 23:42 -03 |
| service   | --all                            | minikube | manuonda | v1.32.0 | 30 Jan 24 23:58 -03 |                     |
| start     |                                  | minikube | manuonda | v1.32.0 | 31 Jan 24 09:10 -03 | 31 Jan 24 09:11 -03 |
| service   | --all                            | minikube | manuonda | v1.32.0 | 31 Jan 24 09:51 -03 |                     |
| stop      |                                  | minikube | manuonda | v1.32.0 | 31 Jan 24 09:56 -03 | 31 Jan 24 09:56 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 31 Jan 24 09:56 -03 | 31 Jan 24 09:56 -03 |
| service   | all                              | minikube | manuonda | v1.32.0 | 31 Jan 24 10:04 -03 |                     |
| service   |                                  | minikube | manuonda | v1.32.0 | 31 Jan 24 10:04 -03 |                     |
| service   | --all                            | minikube | manuonda | v1.32.0 | 31 Jan 24 10:05 -03 |                     |
| service   | msvc-usuarios --url              | minikube | manuonda | v1.32.0 | 31 Jan 24 10:10 -03 | 31 Jan 24 10:10 -03 |
| service   | msvc-usuarios --url              | minikube | manuonda | v1.32.0 | 31 Jan 24 10:12 -03 | 31 Jan 24 10:12 -03 |
| service   | msvc-curso --url                 | minikube | manuonda | v1.32.0 | 31 Jan 24 10:13 -03 |                     |
| service   | msvc-usuarios --url              | minikube | manuonda | v1.32.0 | 31 Jan 24 10:13 -03 | 31 Jan 24 10:13 -03 |
| service   | msvc-curso --url                 | minikube | manuonda | v1.32.0 | 31 Jan 24 10:13 -03 |                     |
| service   | msvc-cursos --url                | minikube | manuonda | v1.32.0 | 31 Jan 24 10:13 -03 |                     |
| service   | msvc-usuarios --url              | minikube | manuonda | v1.32.0 | 31 Jan 24 19:25 -03 | 31 Jan 24 19:25 -03 |
| start     |                                  | minikube | manuonda | v1.32.0 | 03 Feb 24 00:30 -03 | 03 Feb 24 00:31 -03 |
| service   | --url msvc-gateway               | minikube | manuonda | v1.32.0 | 03 Feb 24 00:34 -03 |                     |
| service   | msvc-gateway --url               | minikube | manuonda | v1.32.0 | 03 Feb 24 00:34 -03 |                     |
| service   | msvc-gateway                     | minikube | manuonda | v1.32.0 | 03 Feb 24 00:34 -03 |                     |
| service   | msvc-usuarios                    | minikube | manuonda | v1.32.0 | 03 Feb 24 00:35 -03 | 03 Feb 24 00:35 -03 |
| service   | --all                            | minikube | manuonda | v1.32.0 | 03 Feb 24 00:38 -03 |                     |
| service   | msvc-gateway                     | minikube | manuonda | v1.32.0 | 03 Feb 24 00:40 -03 |                     |
|-----------|----------------------------------|----------|----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/03 00:30:46
Running on machine: star-force
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0203 00:30:46.633066  117911 out.go:296] Setting OutFile to fd 1 ...
I0203 00:30:46.633722  117911 out.go:348] isatty.IsTerminal(1) = true
I0203 00:30:46.633725  117911 out.go:309] Setting ErrFile to fd 2...
I0203 00:30:46.633727  117911 out.go:348] isatty.IsTerminal(2) = true
I0203 00:30:46.634028  117911 root.go:338] Updating PATH: /home/manuonda/.minikube/bin
W0203 00:30:46.634344  117911 root.go:314] Error reading config file at /home/manuonda/.minikube/config/config.json: open /home/manuonda/.minikube/config/config.json: no such file or directory
I0203 00:30:46.634915  117911 out.go:303] Setting JSON to false
I0203 00:30:46.635933  117911 start.go:128] hostinfo: {"hostname":"star-force","uptime":62057,"bootTime":1706868990,"procs":435,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-15-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"2d2fd6d2-354b-4c88-bfd7-8717868bb844"}
I0203 00:30:46.635966  117911 start.go:138] virtualization: kvm host
I0203 00:30:46.636819  117911 out.go:177] üòÑ  minikube v1.32.0 en Ubuntu 22.04
I0203 00:30:46.638850  117911 notify.go:220] Checking for updates...
I0203 00:30:46.639111  117911 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0203 00:30:46.639485  117911 driver.go:378] Setting default libvirt URI to qemu:///system
I0203 00:30:46.735020  117911 docker.go:122] docker version: linux-24.0.5:
I0203 00:30:46.735067  117911 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0203 00:30:46.787666  117911 info.go:266] docker info: {ID:3179519d-5bef-48ea-a8f4-bbca8bc48cf7 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:123 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-02-03 00:30:46.783060137 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-15-generic OperatingSystem:Ubuntu Core 22 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:33401581568 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:star-force Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.3]] Warnings:<nil>}}
I0203 00:30:46.787710  117911 docker.go:295] overlay module found
I0203 00:30:46.788423  117911 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0203 00:30:46.789542  117911 start.go:298] selected driver: docker
I0203 00:30:46.789544  117911 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:7900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/manuonda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0203 00:30:46.789576  117911 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0203 00:30:46.789622  117911 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0203 00:30:46.871417  117911 info.go:266] docker info: {ID:3179519d-5bef-48ea-a8f4-bbca8bc48cf7 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:123 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-02-03 00:30:46.867177051 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-15-generic OperatingSystem:Ubuntu Core 22 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:33401581568 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:star-force Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.3]] Warnings:<nil>}}
I0203 00:30:46.872189  117911 cni.go:84] Creating CNI manager for ""
I0203 00:30:46.872196  117911 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0203 00:30:46.872202  117911 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:7900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/manuonda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0203 00:30:46.873110  117911 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0203 00:30:46.874226  117911 cache.go:121] Beginning downloading kic base image for docker with docker
I0203 00:30:46.874812  117911 out.go:177] üöú  Pulling base image ...
I0203 00:30:46.875511  117911 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0203 00:30:46.875619  117911 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0203 00:30:46.875637  117911 preload.go:148] Found local preload: /home/manuonda/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0203 00:30:46.875640  117911 cache.go:56] Caching tarball of preloaded images
I0203 00:30:46.875687  117911 preload.go:174] Found /home/manuonda/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0203 00:30:46.875691  117911 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0203 00:30:46.875738  117911 profile.go:148] Saving config to /home/manuonda/.minikube/profiles/minikube/config.json ...
I0203 00:30:46.988265  117911 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0203 00:30:46.988277  117911 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0203 00:30:46.988287  117911 cache.go:194] Successfully downloaded all kic artifacts
I0203 00:30:46.988305  117911 start.go:365] acquiring machines lock for minikube: {Name:mk9ddb26da42098693e633c45036f27066e01d67 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0203 00:30:46.988365  117911 start.go:369] acquired machines lock for "minikube" in 51.103¬µs
I0203 00:30:46.988376  117911 start.go:96] Skipping create...Using existing machine configuration
I0203 00:30:46.988378  117911 fix.go:54] fixHost starting: 
I0203 00:30:46.988518  117911 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0203 00:30:47.076896  117911 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0203 00:30:47.076908  117911 fix.go:128] unexpected machine state, will restart: <nil>
I0203 00:30:47.077624  117911 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0203 00:30:47.078764  117911 cli_runner.go:164] Run: docker start minikube
I0203 00:30:47.570363  117911 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0203 00:30:47.643862  117911 kic.go:430] container "minikube" state is running.
I0203 00:30:47.644061  117911 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0203 00:30:47.674568  117911 profile.go:148] Saving config to /home/manuonda/.minikube/profiles/minikube/config.json ...
I0203 00:30:47.674666  117911 machine.go:88] provisioning docker machine ...
I0203 00:30:47.674685  117911 ubuntu.go:169] provisioning hostname "minikube"
I0203 00:30:47.674715  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:47.707240  117911 main.go:141] libmachine: Using SSH client type: native
I0203 00:30:47.707641  117911 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0203 00:30:47.707646  117911 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0203 00:30:47.707916  117911 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:43178->127.0.0.1:32772: read: connection reset by peer
I0203 00:30:50.876393  117911 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0203 00:30:50.876557  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:50.965482  117911 main.go:141] libmachine: Using SSH client type: native
I0203 00:30:50.965680  117911 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0203 00:30:50.965686  117911 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0203 00:30:51.092824  117911 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0203 00:30:51.092889  117911 ubuntu.go:175] set auth options {CertDir:/home/manuonda/.minikube CaCertPath:/home/manuonda/.minikube/certs/ca.pem CaPrivateKeyPath:/home/manuonda/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/manuonda/.minikube/machines/server.pem ServerKeyPath:/home/manuonda/.minikube/machines/server-key.pem ClientKeyPath:/home/manuonda/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/manuonda/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/manuonda/.minikube}
I0203 00:30:51.092918  117911 ubuntu.go:177] setting up certificates
I0203 00:30:51.092933  117911 provision.go:83] configureAuth start
I0203 00:30:51.093060  117911 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0203 00:30:51.188290  117911 provision.go:138] copyHostCerts
I0203 00:30:51.188420  117911 exec_runner.go:144] found /home/manuonda/.minikube/ca.pem, removing ...
I0203 00:30:51.188425  117911 exec_runner.go:203] rm: /home/manuonda/.minikube/ca.pem
I0203 00:30:51.188457  117911 exec_runner.go:151] cp: /home/manuonda/.minikube/certs/ca.pem --> /home/manuonda/.minikube/ca.pem (1082 bytes)
I0203 00:30:51.188624  117911 exec_runner.go:144] found /home/manuonda/.minikube/cert.pem, removing ...
I0203 00:30:51.188627  117911 exec_runner.go:203] rm: /home/manuonda/.minikube/cert.pem
I0203 00:30:51.188638  117911 exec_runner.go:151] cp: /home/manuonda/.minikube/certs/cert.pem --> /home/manuonda/.minikube/cert.pem (1127 bytes)
I0203 00:30:51.188787  117911 exec_runner.go:144] found /home/manuonda/.minikube/key.pem, removing ...
I0203 00:30:51.188789  117911 exec_runner.go:203] rm: /home/manuonda/.minikube/key.pem
I0203 00:30:51.188815  117911 exec_runner.go:151] cp: /home/manuonda/.minikube/certs/key.pem --> /home/manuonda/.minikube/key.pem (1675 bytes)
I0203 00:30:51.189018  117911 provision.go:112] generating server cert: /home/manuonda/.minikube/machines/server.pem ca-key=/home/manuonda/.minikube/certs/ca.pem private-key=/home/manuonda/.minikube/certs/ca-key.pem org=manuonda.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0203 00:30:51.295786  117911 provision.go:172] copyRemoteCerts
I0203 00:30:51.295815  117911 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0203 00:30:51.295839  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:51.395930  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:30:51.497355  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0203 00:30:51.562023  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0203 00:30:51.618574  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0203 00:30:51.661553  117911 provision.go:86] duration metric: configureAuth took 568.604616ms
I0203 00:30:51.661576  117911 ubuntu.go:193] setting minikube options for container-runtime
I0203 00:30:51.661841  117911 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0203 00:30:51.661949  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:51.751830  117911 main.go:141] libmachine: Using SSH client type: native
I0203 00:30:51.752029  117911 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0203 00:30:51.752033  117911 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0203 00:30:51.877456  117911 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0203 00:30:51.877476  117911 ubuntu.go:71] root file system type: overlay
I0203 00:30:51.877752  117911 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0203 00:30:51.877880  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:51.957399  117911 main.go:141] libmachine: Using SSH client type: native
I0203 00:30:51.957605  117911 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0203 00:30:51.957636  117911 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0203 00:30:52.094309  117911 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0203 00:30:52.094378  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:52.165430  117911 main.go:141] libmachine: Using SSH client type: native
I0203 00:30:52.165628  117911 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0203 00:30:52.165635  117911 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0203 00:30:52.300883  117911 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0203 00:30:52.300906  117911 machine.go:91] provisioned docker machine in 4.62622958s
I0203 00:30:52.300922  117911 start.go:300] post-start starting for "minikube" (driver="docker")
I0203 00:30:52.300941  117911 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0203 00:30:52.301091  117911 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0203 00:30:52.301206  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:52.371845  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:30:52.465513  117911 ssh_runner.go:195] Run: cat /etc/os-release
I0203 00:30:52.472083  117911 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0203 00:30:52.472139  117911 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0203 00:30:52.472162  117911 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0203 00:30:52.472186  117911 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0203 00:30:52.472203  117911 filesync.go:126] Scanning /home/manuonda/.minikube/addons for local assets ...
I0203 00:30:52.472468  117911 filesync.go:126] Scanning /home/manuonda/.minikube/files for local assets ...
I0203 00:30:52.472643  117911 start.go:303] post-start completed in 171.708477ms
I0203 00:30:52.472744  117911 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0203 00:30:52.472846  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:52.552556  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:30:52.638824  117911 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0203 00:30:52.649548  117911 fix.go:56] fixHost completed within 5.661158213s
I0203 00:30:52.649578  117911 start.go:83] releasing machines lock for "minikube", held for 5.661199807s
I0203 00:30:52.649738  117911 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0203 00:30:52.756965  117911 ssh_runner.go:195] Run: cat /version.json
I0203 00:30:52.756996  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:52.757001  117911 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0203 00:30:52.757033  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:30:52.832444  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:30:52.832814  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:30:53.625252  117911 ssh_runner.go:195] Run: systemctl --version
I0203 00:30:53.638972  117911 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0203 00:30:53.647531  117911 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0203 00:30:53.668122  117911 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0203 00:30:53.668151  117911 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0203 00:30:53.674022  117911 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0203 00:30:53.674031  117911 start.go:472] detecting cgroup driver to use...
I0203 00:30:53.674043  117911 detect.go:199] detected "systemd" cgroup driver on host os
I0203 00:30:53.674090  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0203 00:30:53.682806  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0203 00:30:53.688229  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0203 00:30:53.693351  117911 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0203 00:30:53.693374  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0203 00:30:53.698404  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0203 00:30:53.703456  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0203 00:30:53.708595  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0203 00:30:53.713798  117911 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0203 00:30:53.718628  117911 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0203 00:30:53.723941  117911 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0203 00:30:53.729632  117911 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0203 00:30:53.734191  117911 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0203 00:30:53.784192  117911 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0203 00:30:53.853806  117911 start.go:472] detecting cgroup driver to use...
I0203 00:30:53.853827  117911 detect.go:199] detected "systemd" cgroup driver on host os
I0203 00:30:53.853850  117911 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0203 00:30:53.860066  117911 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0203 00:30:53.860096  117911 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0203 00:30:53.867473  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0203 00:30:53.877189  117911 ssh_runner.go:195] Run: which cri-dockerd
I0203 00:30:53.879052  117911 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0203 00:30:53.883479  117911 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0203 00:30:53.892881  117911 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0203 00:30:53.959507  117911 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0203 00:30:54.013117  117911 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0203 00:30:54.013176  117911 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0203 00:30:54.022492  117911 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0203 00:30:54.085386  117911 ssh_runner.go:195] Run: sudo systemctl restart docker
I0203 00:30:55.557349  117911 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.471949381s)
I0203 00:30:55.557376  117911 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0203 00:30:55.608770  117911 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0203 00:30:55.660881  117911 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0203 00:30:55.710101  117911 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0203 00:30:55.756539  117911 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0203 00:30:55.808281  117911 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0203 00:30:55.871120  117911 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0203 00:30:56.022528  117911 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0203 00:30:56.022585  117911 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0203 00:30:56.024384  117911 start.go:540] Will wait 60s for crictl version
I0203 00:30:56.024409  117911 ssh_runner.go:195] Run: which crictl
I0203 00:30:56.026003  117911 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0203 00:30:56.095905  117911 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0203 00:30:56.095943  117911 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0203 00:30:56.143622  117911 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0203 00:30:56.154957  117911 out.go:204] üê≥  Preparando Kubernetes v1.28.3 en Docker 24.0.7...
I0203 00:30:56.155018  117911 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0203 00:30:56.241170  117911 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0203 00:30:56.242973  117911 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0203 00:30:56.248744  117911 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0203 00:30:56.248772  117911 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0203 00:30:56.259348  117911 docker.go:671] Got preloaded images: -- stdout --
manuonda/curso-docker-msc-usuarios:latest
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
<none>:<none>
<none>:<none>
<none>:<none>
manuonda/curso-docker-msc-curso:latest
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
postgres:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0203 00:30:56.259355  117911 docker.go:601] Images already preloaded, skipping extraction
I0203 00:30:56.259388  117911 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0203 00:30:56.271277  117911 docker.go:671] Got preloaded images: -- stdout --
manuonda/curso-docker-msc-usuarios:latest
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
manuonda/curso-docker-msc-usuarios:<none>
<none>:<none>
<none>:<none>
<none>:<none>
manuonda/curso-docker-msc-curso:latest
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
postgres:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0203 00:30:56.271284  117911 cache_images.go:84] Images are preloaded, skipping loading
I0203 00:30:56.271313  117911 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0203 00:30:56.372636  117911 cni.go:84] Creating CNI manager for ""
I0203 00:30:56.372644  117911 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0203 00:30:56.372854  117911 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0203 00:30:56.372868  117911 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0203 00:30:56.372927  117911 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0203 00:30:56.372972  117911 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0203 00:30:56.372997  117911 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0203 00:30:56.378205  117911 binaries.go:44] Found k8s binaries, skipping transfer
I0203 00:30:56.378233  117911 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0203 00:30:56.383203  117911 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0203 00:30:56.399010  117911 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0203 00:30:56.432190  117911 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0203 00:30:56.466463  117911 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0203 00:30:56.471675  117911 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0203 00:30:56.490295  117911 certs.go:56] Setting up /home/manuonda/.minikube/profiles/minikube for IP: 192.168.49.2
I0203 00:30:56.490320  117911 certs.go:190] acquiring lock for shared ca certs: {Name:mk264a120d9c17c83a576eba5d1f4e69374bc5b7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0203 00:30:56.490673  117911 certs.go:199] skipping minikubeCA CA generation: /home/manuonda/.minikube/ca.key
I0203 00:30:56.490877  117911 certs.go:199] skipping proxyClientCA CA generation: /home/manuonda/.minikube/proxy-client-ca.key
I0203 00:30:56.491087  117911 certs.go:315] skipping minikube-user signed cert generation: /home/manuonda/.minikube/profiles/minikube/client.key
I0203 00:30:56.491275  117911 certs.go:315] skipping minikube signed cert generation: /home/manuonda/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0203 00:30:56.491417  117911 certs.go:315] skipping aggregator signed cert generation: /home/manuonda/.minikube/profiles/minikube/proxy-client.key
I0203 00:30:56.491558  117911 certs.go:437] found cert: /home/manuonda/.minikube/certs/home/manuonda/.minikube/certs/ca-key.pem (1675 bytes)
I0203 00:30:56.491594  117911 certs.go:437] found cert: /home/manuonda/.minikube/certs/home/manuonda/.minikube/certs/ca.pem (1082 bytes)
I0203 00:30:56.491629  117911 certs.go:437] found cert: /home/manuonda/.minikube/certs/home/manuonda/.minikube/certs/cert.pem (1127 bytes)
I0203 00:30:56.491658  117911 certs.go:437] found cert: /home/manuonda/.minikube/certs/home/manuonda/.minikube/certs/key.pem (1675 bytes)
I0203 00:30:56.492790  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0203 00:30:56.508330  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0203 00:30:56.533488  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0203 00:30:56.570062  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0203 00:30:56.618424  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0203 00:30:56.667612  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0203 00:30:56.720959  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0203 00:30:56.762637  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0203 00:30:56.811520  117911 ssh_runner.go:362] scp /home/manuonda/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0203 00:30:56.867642  117911 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0203 00:30:56.890854  117911 ssh_runner.go:195] Run: openssl version
I0203 00:30:56.895948  117911 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0203 00:30:56.902614  117911 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0203 00:30:56.905749  117911 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Dec 16 13:11 /usr/share/ca-certificates/minikubeCA.pem
I0203 00:30:56.905783  117911 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0203 00:30:56.909172  117911 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0203 00:30:56.913851  117911 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0203 00:30:56.915544  117911 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0203 00:30:56.918784  117911 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0203 00:30:56.922000  117911 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0203 00:30:56.925162  117911 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0203 00:30:56.928313  117911 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0203 00:30:56.931437  117911 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0203 00:30:56.934512  117911 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:7900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/manuonda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0203 00:30:56.934566  117911 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0203 00:30:56.943110  117911 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0203 00:30:56.947992  117911 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0203 00:30:56.947996  117911 kubeadm.go:636] restartCluster start
I0203 00:30:56.948021  117911 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0203 00:30:56.952378  117911 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0203 00:30:56.953018  117911 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0203 00:30:56.956103  117911 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0203 00:30:56.960623  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:56.960639  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:56.966545  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:56.966550  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:56.966568  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:56.971875  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:57.472313  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:57.472460  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:57.484076  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:57.972594  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:57.972708  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:57.998611  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:58.472955  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:58.473049  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:58.484894  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:58.972538  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:58.972650  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:58.998883  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:59.472084  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:59.472127  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:59.478270  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:30:59.972360  117911 api_server.go:166] Checking apiserver status ...
I0203 00:30:59.972493  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:30:59.985840  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:00.472992  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:00.473135  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:00.484766  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:00.972728  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:00.972841  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:00.984823  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:01.472844  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:01.472965  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:01.498430  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:01.972630  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:01.972767  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:01.998063  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:02.472240  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:02.472360  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:02.486089  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:02.972569  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:02.972677  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:02.985079  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:03.472789  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:03.472933  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:03.498085  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:03.972806  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:03.972944  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:03.984709  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:04.472299  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:04.472394  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:04.484007  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:04.972701  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:04.972850  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:04.984741  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:05.472478  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:05.472631  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:05.486424  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:05.972892  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:05.973013  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:05.987924  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:06.472686  117911 api_server.go:166] Checking apiserver status ...
I0203 00:31:06.472832  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0203 00:31:06.484863  117911 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0203 00:31:06.961506  117911 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0203 00:31:06.961532  117911 kubeadm.go:1128] stopping kube-system containers ...
I0203 00:31:06.961706  117911 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0203 00:31:06.979337  117911 docker.go:469] Stopping containers: [fb59432f78c6 048b82aaffa2 bf6b488216d2 e002b8b7b1c7 162d9a0bf87c 9a2c0a588c6d 5df4c2ef9dec 41136a3d7fca 37da32f98738 8acccab6325a b7afdd27a98a d588959f3ade 5bf060e6deba 37a888b8f010 ee5c8266fd1b 7742706b04f0 6efe9d633c3d cd9578aab0df b938190a15ef ac9fcedea7bf 0272cb0a67de e35f8cf34af4 365ac859aece d28ef0793c4c d4bcb8f8bfb8 6c25f1fe1553 3aa1282e6ad9]
I0203 00:31:06.979374  117911 ssh_runner.go:195] Run: docker stop fb59432f78c6 048b82aaffa2 bf6b488216d2 e002b8b7b1c7 162d9a0bf87c 9a2c0a588c6d 5df4c2ef9dec 41136a3d7fca 37da32f98738 8acccab6325a b7afdd27a98a d588959f3ade 5bf060e6deba 37a888b8f010 ee5c8266fd1b 7742706b04f0 6efe9d633c3d cd9578aab0df b938190a15ef ac9fcedea7bf 0272cb0a67de e35f8cf34af4 365ac859aece d28ef0793c4c d4bcb8f8bfb8 6c25f1fe1553 3aa1282e6ad9
I0203 00:31:06.988506  117911 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0203 00:31:06.994907  117911 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0203 00:31:06.999370  117911 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Jan 31 02:42 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Jan 31 12:56 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Jan 31 02:42 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan 31 12:56 /etc/kubernetes/scheduler.conf

I0203 00:31:06.999389  117911 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0203 00:31:07.004130  117911 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0203 00:31:07.008646  117911 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0203 00:31:07.013371  117911 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0203 00:31:07.013393  117911 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0203 00:31:07.017848  117911 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0203 00:31:07.022413  117911 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0203 00:31:07.022436  117911 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0203 00:31:07.026775  117911 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0203 00:31:07.031424  117911 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0203 00:31:07.031431  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0203 00:31:07.123475  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0203 00:31:07.713394  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0203 00:31:07.816438  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0203 00:31:07.844923  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0203 00:31:07.868738  117911 api_server.go:52] waiting for apiserver process to appear ...
I0203 00:31:07.868766  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0203 00:31:07.874107  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0203 00:31:08.380071  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0203 00:31:08.879749  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0203 00:31:09.380153  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0203 00:31:09.387008  117911 api_server.go:72] duration metric: took 1.518268784s to wait for apiserver process to appear ...
I0203 00:31:09.387016  117911 api_server.go:88] waiting for apiserver healthz status ...
I0203 00:31:09.387025  117911 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0203 00:31:10.665399  117911 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0203 00:31:10.665414  117911 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0203 00:31:10.665426  117911 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0203 00:31:10.698662  117911 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0203 00:31:10.698671  117911 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0203 00:31:11.198816  117911 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0203 00:31:11.203973  117911 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0203 00:31:11.203981  117911 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0203 00:31:11.699506  117911 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0203 00:31:11.701994  117911 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0203 00:31:11.702003  117911 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0203 00:31:12.198976  117911 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0203 00:31:12.201866  117911 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0203 00:31:12.205699  117911 api_server.go:141] control plane version: v1.28.3
I0203 00:31:12.205705  117911 api_server.go:131] duration metric: took 2.818686434s to wait for apiserver health ...
I0203 00:31:12.205708  117911 cni.go:84] Creating CNI manager for ""
I0203 00:31:12.205715  117911 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0203 00:31:12.206443  117911 out.go:177] üîó  Configurando CNI bridge CNI ...
I0203 00:31:12.207082  117911 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0203 00:31:12.211841  117911 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0203 00:31:12.221095  117911 system_pods.go:43] waiting for kube-system pods to appear ...
I0203 00:31:12.225176  117911 system_pods.go:59] 7 kube-system pods found
I0203 00:31:12.225184  117911 system_pods.go:61] "coredns-5dd5756b68-2rks9" [7deed438-53ae-47cb-97fb-d38ea3b2dded] Running
I0203 00:31:12.225187  117911 system_pods.go:61] "etcd-minikube" [9678a9cc-8c31-41cc-becf-a53ddea19546] Running
I0203 00:31:12.225189  117911 system_pods.go:61] "kube-apiserver-minikube" [5bfba419-a282-4beb-ba90-bd4274c0d19c] Running
I0203 00:31:12.225191  117911 system_pods.go:61] "kube-controller-manager-minikube" [f8af982d-c61a-4d3a-abe0-ffa36b06f1d5] Running
I0203 00:31:12.225193  117911 system_pods.go:61] "kube-proxy-szgdk" [dd35841d-0afa-45a5-bee3-008439993346] Running
I0203 00:31:12.225194  117911 system_pods.go:61] "kube-scheduler-minikube" [1b0851f5-7cf7-4b62-b57c-d4086e281915] Running
I0203 00:31:12.225196  117911 system_pods.go:61] "storage-provisioner" [7fa3e48c-c7ff-4ca0-b8df-20261283ea0f] Running
I0203 00:31:12.225198  117911 system_pods.go:74] duration metric: took 4.098067ms to wait for pod list to return data ...
I0203 00:31:12.225201  117911 node_conditions.go:102] verifying NodePressure condition ...
I0203 00:31:12.226732  117911 node_conditions.go:122] node storage ephemeral capacity is 143074460Ki
I0203 00:31:12.226738  117911 node_conditions.go:123] node cpu capacity is 12
I0203 00:31:12.226746  117911 node_conditions.go:105] duration metric: took 1.54351ms to run NodePressure ...
I0203 00:31:12.226753  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0203 00:31:12.300355  117911 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0203 00:31:12.303933  117911 ops.go:34] apiserver oom_adj: -16
I0203 00:31:12.303938  117911 kubeadm.go:640] restartCluster took 15.355939651s
I0203 00:31:12.303942  117911 kubeadm.go:406] StartCluster complete in 15.369431957s
I0203 00:31:12.303949  117911 settings.go:142] acquiring lock: {Name:mkc11889a3a918a18dd0c112d0e4d9945c5aa025 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0203 00:31:12.304000  117911 settings.go:150] Updating kubeconfig:  /home/manuonda/.kube/config
I0203 00:31:12.304312  117911 lock.go:35] WriteFile acquiring /home/manuonda/.kube/config: {Name:mkf55b0c24c377809ffeb39ecbf758963680b061 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0203 00:31:12.304411  117911 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0203 00:31:12.304550  117911 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0203 00:31:12.304530  117911 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0203 00:31:12.304573  117911 addons.go:69] Setting dashboard=true in profile "minikube"
I0203 00:31:12.304573  117911 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0203 00:31:12.304578  117911 addons.go:231] Setting addon dashboard=true in "minikube"
I0203 00:31:12.304578  117911 addons.go:69] Setting default-storageclass=true in profile "minikube"
W0203 00:31:12.304580  117911 addons.go:240] addon dashboard should already be in state true
I0203 00:31:12.304581  117911 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0203 00:31:12.304585  117911 addons.go:240] addon storage-provisioner should already be in state true
I0203 00:31:12.304585  117911 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0203 00:31:12.304599  117911 host.go:66] Checking if "minikube" exists ...
I0203 00:31:12.304608  117911 host.go:66] Checking if "minikube" exists ...
I0203 00:31:12.304729  117911 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0203 00:31:12.304765  117911 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0203 00:31:12.304854  117911 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0203 00:31:12.306566  117911 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0203 00:31:12.306584  117911 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0203 00:31:12.307452  117911 out.go:177] üîé  Verifying Kubernetes components...
I0203 00:31:12.308133  117911 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0203 00:31:12.389840  117911 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0203 00:31:12.389849  117911 addons.go:240] addon default-storageclass should already be in state true
I0203 00:31:12.389863  117911 host.go:66] Checking if "minikube" exists ...
I0203 00:31:12.390073  117911 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0203 00:31:12.391364  117911 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0203 00:31:12.392379  117911 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0203 00:31:12.393445  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0203 00:31:12.393452  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0203 00:31:12.394175  117911 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0203 00:31:12.393484  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:31:12.395123  117911 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0203 00:31:12.395135  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0203 00:31:12.395236  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:31:12.429355  117911 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0203 00:31:12.429364  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0203 00:31:12.429409  117911 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0203 00:31:12.429951  117911 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0203 00:31:12.429955  117911 api_server.go:52] waiting for apiserver process to appear ...
I0203 00:31:12.429985  117911 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0203 00:31:12.433326  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:31:12.433326  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:31:12.436933  117911 api_server.go:72] duration metric: took 130.33406ms to wait for apiserver process to appear ...
I0203 00:31:12.436942  117911 api_server.go:88] waiting for apiserver healthz status ...
I0203 00:31:12.436953  117911 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0203 00:31:12.440407  117911 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0203 00:31:12.440947  117911 api_server.go:141] control plane version: v1.28.3
I0203 00:31:12.440953  117911 api_server.go:131] duration metric: took 4.007863ms to wait for apiserver health ...
I0203 00:31:12.440957  117911 system_pods.go:43] waiting for kube-system pods to appear ...
I0203 00:31:12.443688  117911 system_pods.go:59] 7 kube-system pods found
I0203 00:31:12.443695  117911 system_pods.go:61] "coredns-5dd5756b68-2rks9" [7deed438-53ae-47cb-97fb-d38ea3b2dded] Running
I0203 00:31:12.443698  117911 system_pods.go:61] "etcd-minikube" [9678a9cc-8c31-41cc-becf-a53ddea19546] Running
I0203 00:31:12.443700  117911 system_pods.go:61] "kube-apiserver-minikube" [5bfba419-a282-4beb-ba90-bd4274c0d19c] Running
I0203 00:31:12.443703  117911 system_pods.go:61] "kube-controller-manager-minikube" [f8af982d-c61a-4d3a-abe0-ffa36b06f1d5] Running
I0203 00:31:12.443705  117911 system_pods.go:61] "kube-proxy-szgdk" [dd35841d-0afa-45a5-bee3-008439993346] Running
I0203 00:31:12.443707  117911 system_pods.go:61] "kube-scheduler-minikube" [1b0851f5-7cf7-4b62-b57c-d4086e281915] Running
I0203 00:31:12.443708  117911 system_pods.go:61] "storage-provisioner" [7fa3e48c-c7ff-4ca0-b8df-20261283ea0f] Running
I0203 00:31:12.443711  117911 system_pods.go:74] duration metric: took 2.750869ms to wait for pod list to return data ...
I0203 00:31:12.443714  117911 kubeadm.go:581] duration metric: took 137.118527ms to wait for : map[apiserver:true system_pods:true] ...
I0203 00:31:12.443720  117911 node_conditions.go:102] verifying NodePressure condition ...
I0203 00:31:12.445018  117911 node_conditions.go:122] node storage ephemeral capacity is 143074460Ki
I0203 00:31:12.445025  117911 node_conditions.go:123] node cpu capacity is 12
I0203 00:31:12.445031  117911 node_conditions.go:105] duration metric: took 1.309418ms to run NodePressure ...
I0203 00:31:12.445038  117911 start.go:228] waiting for startup goroutines ...
I0203 00:31:12.488233  117911 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/manuonda/.minikube/machines/minikube/id_rsa Username:docker}
I0203 00:31:12.513301  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0203 00:31:12.513306  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0203 00:31:12.513307  117911 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0203 00:31:12.523086  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0203 00:31:12.523093  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0203 00:31:12.533036  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0203 00:31:12.533042  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0203 00:31:12.543346  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0203 00:31:12.543351  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0203 00:31:12.553622  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0203 00:31:12.553632  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0203 00:31:12.564003  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0203 00:31:12.564011  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0203 00:31:12.568993  117911 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0203 00:31:12.574100  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0203 00:31:12.574107  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0203 00:31:12.584087  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0203 00:31:12.584094  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0203 00:31:12.598104  117911 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0203 00:31:12.598112  117911 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0203 00:31:12.609611  117911 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0203 00:31:12.978661  117911 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0203 00:31:12.979314  117911 out.go:177] üåü  Complementos habilitados: storage-provisioner, default-storageclass, dashboard
I0203 00:31:12.979891  117911 addons.go:502] enable addons completed in 675.367163ms: enabled=[storage-provisioner default-storageclass dashboard]
I0203 00:31:12.979901  117911 start.go:233] waiting for cluster config update ...
I0203 00:31:12.979905  117911 start.go:242] writing updated cluster config ...
I0203 00:31:12.980017  117911 ssh_runner.go:195] Run: rm -f paused
I0203 00:31:13.093267  117911 start.go:600] kubectl: 1.29.0, cluster: 1.28.3 (minor skew: 1)
I0203 00:31:13.093969  117911 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"msvc-usuarios-6ff88998d-bm7mp_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6b0c6037442e26533c33793bfb7ec2b0b1eaaf2234da88435595f27b7a2b3df0\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgresql-container-598784c45b-pv6dl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a6b4629877f581b514e4852a3b801776a434f352b6b85bc468c305c6fbe183ae\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgresql-container-598784c45b-pv6dl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9fdadf2f937f06ff819b840974a275dd9851206ef0266632ef9cb8e09563231e\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-x6znq_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e58b6c18e698ea023b28f48409e8e8af972ba3c2cc2d90afd1d73f5c90be3194\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-x6znq_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"79960cbc64ba920a6e23c9e8eabe40b92e48319d2c59cf43410ae232902582a3\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-2rks9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"162d9a0bf87cb8e887c35570fd4968c60da9aa58ab5c8735592512c6ad955a1d\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-2rks9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b938190a15ef5bca1b20cfa993beaaaed98c335994425788f730147c2decff7c\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"msvc-cursos-5599cf6bbb-5n4r7_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"93484102befc87e47a5d9287f10678607141b935ee7cf5cb06c000cbc2753235\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-hjsds_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e5becc283da6db149dc97c4f4b7cad31a000f79a8ee18a3ab12ff05b3f082adc\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"msvc-usuarios-6ff88998d-mxv7z_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"08bb7efb1b2c1653babd5d21ece8af311af588d26dd9ef4308106aa79eaae9cb\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"msvc-usuarios-6ff88998d-8lxfd_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c8d0a9ac49c360c0f74b92ac5c42cede76a02a913c3092a9358df2501322b905\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"msvc-usuarios-6ff88998d-mxhlg_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"80fc0af02945bda36c3fdde3b822c55fe565da01a00376d695cba7fb54d537a0\""
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"6c25f1fe15536beeac112afbaa4c8a0cd43a4e44c76f2592478f0b2d8d082e13\". Proceed without further sandbox information."
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"d4bcb8f8bfb8fe083879f7b0f024c872136b0a9abef781da5f92a98f81ccf0dc\". Proceed without further sandbox information."
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"d28ef0793c4cee362d18b6a5e282b876c385b78d578536cfe769a0196b3938a7\". Proceed without further sandbox information."
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"3aa1282e6ad9a7a84d1019bb1d0207cc27b6225d70caf7a6fa2baa76836abc3e\". Proceed without further sandbox information."
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7ab846935ad1995f05aafacf35ba1db645808a3b645ca1c37bbad49da69d51f0/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c259350bdd4a41d9c24733f094e93db82eb24ef5b4e81784669c035ae68baab8/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/440cc924f64aadcaac833845402af69c00121c123015cce1b066594bdc85d75c/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:08 minikube cri-dockerd[1099]: time="2024-02-03T03:31:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e70ba1d957391fa1448d30ce7874ad34bc5e75a10ce5cd43561b8fcca5ce4034/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:09 minikube cri-dockerd[1099]: time="2024-02-03T03:31:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgresql-container-598784c45b-pv6dl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a6b4629877f581b514e4852a3b801776a434f352b6b85bc468c305c6fbe183ae\""
Feb 03 03:31:09 minikube cri-dockerd[1099]: time="2024-02-03T03:31:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-2rks9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"162d9a0bf87cb8e887c35570fd4968c60da9aa58ab5c8735592512c6ad955a1d\""
Feb 03 03:31:09 minikube cri-dockerd[1099]: time="2024-02-03T03:31:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-x6znq_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e58b6c18e698ea023b28f48409e8e8af972ba3c2cc2d90afd1d73f5c90be3194\""
Feb 03 03:31:09 minikube cri-dockerd[1099]: time="2024-02-03T03:31:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"msvc-cursos-5599cf6bbb-5n4r7_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"93484102befc87e47a5d9287f10678607141b935ee7cf5cb06c000cbc2753235\""
Feb 03 03:31:09 minikube cri-dockerd[1099]: time="2024-02-03T03:31:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-hjsds_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e5becc283da6db149dc97c4f4b7cad31a000f79a8ee18a3ab12ff05b3f082adc\""
Feb 03 03:31:10 minikube cri-dockerd[1099]: time="2024-02-03T03:31:10Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa53e1902f1b903f263a8c0f969a89a1e96c4dc1da678ba6c34e00f0eb80624f/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/484c4482c29907507b72cbd1ef6ced1fb96f7a17fa200a380590565ddf991d71/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dc035e2312511e39130913994a90898cf7cf3b2fde978b69a92c150d40f0be78/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9919c314a3b4a847f3f325c7b150698b5e8f86f6c59fe7dd232e5675fe50456a/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40daf3875ef19d467f31f1f4c1c5717fe8618108de66e05b822661adb78e3b54/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/509cecac3293d76c728fe9318d7b1799919eb0168b9d60c1eb22b1a1f25150da/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0692eb9728c41a86404d4fcbe1b54211df895369d44bb2534853fb580ba6c85f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d166df4714cfd77ae4a378998dadf920e20f88421114fa96baf1fe7636a76521/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a4e9bb974c8f71fcc08dedd8893d14f9ce31633d2971dd68b39d94ed455aea6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/590fc0b86a472725ef1e044791e10fbb21106c775f81c0919d6b3921d0ed92ed/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 03 03:31:13 minikube cri-dockerd[1099]: time="2024-02-03T03:31:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ae8465c8ec379c5a00627bb319c9f3bf748d4224c4768c6c8ead4cb68d066426/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:15 minikube cri-dockerd[1099]: time="2024-02-03T03:31:15Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:31:17 minikube cri-dockerd[1099]: time="2024-02-03T03:31:17Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:31:20 minikube cri-dockerd[1099]: time="2024-02-03T03:31:20Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:31:22 minikube cri-dockerd[1099]: time="2024-02-03T03:31:22Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-curso:latest: Status: Image is up to date for manuonda/curso-docker-msc-curso:latest"
Feb 03 03:31:24 minikube cri-dockerd[1099]: time="2024-02-03T03:31:24Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:31:30 minikube dockerd[838]: time="2024-02-03T03:31:30.877258296Z" level=info msg="ignoring event" container=197491e56e835a580acb094a31eb29087d8637a2fe7780cb4ee7504190a16054 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:31 minikube dockerd[838]: time="2024-02-03T03:31:31.820853711Z" level=info msg="ignoring event" container=59ea22d1d048c20f75badc67aa386aa9149bd4ab9d44b5e21e064d2bcf0dbfc0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:32 minikube dockerd[838]: time="2024-02-03T03:31:32.815897016Z" level=info msg="ignoring event" container=9ef3f60e249ec3b8b57c6f7ebd0c8eb92b9479c0db7919db92d293a4166568be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:34 minikube dockerd[838]: time="2024-02-03T03:31:34.540537170Z" level=info msg="ignoring event" container=4645ee48eb717ccf9cf0074bc61d6e440df6301bc26cf769417193b7fd918d2e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:37 minikube cri-dockerd[1099]: time="2024-02-03T03:31:37Z" level=info msg="Pulling image postgres:latest: b7a5cd7c9b9a: Downloading [>                                                  ]  2.146MB/108.6MB"
Feb 03 03:31:38 minikube dockerd[838]: time="2024-02-03T03:31:38.581564126Z" level=info msg="ignoring event" container=4ee9d58f002deaa80f00efea87f065a037b4a8ce4b4b3767dfb4401392b34daf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:39 minikube cri-dockerd[1099]: time="2024-02-03T03:31:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/73a17f27283457c1d27112569f112a1f3040508126dd389e78a9732aa00c5d39/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 03 03:31:43 minikube dockerd[838]: time="2024-02-03T03:31:43.648888182Z" level=info msg="ignoring event" container=c0ae130285d52751528251dbfc016fc9c6967a6471cfca32b8f12782983f6a6e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:43 minikube dockerd[838]: time="2024-02-03T03:31:43.746295543Z" level=info msg="ignoring event" container=11ebc8a26a1314f98cf341c0d63b167ee0fa14f7c48aea86cfbffd99189d7838 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 03 03:31:47 minikube cri-dockerd[1099]: time="2024-02-03T03:31:47Z" level=info msg="Pulling image postgres:latest: b7a5cd7c9b9a: Downloading [====================>                              ]  45.14MB/108.6MB"
Feb 03 03:31:57 minikube cri-dockerd[1099]: time="2024-02-03T03:31:57Z" level=info msg="Pulling image postgres:latest: b7a5cd7c9b9a: Downloading [============================================>      ]  96.85MB/108.6MB"
Feb 03 03:32:04 minikube cri-dockerd[1099]: time="2024-02-03T03:32:04Z" level=info msg="Stop pulling image postgres:latest: Status: Downloaded newer image for postgres:latest"
Feb 03 03:32:15 minikube cri-dockerd[1099]: time="2024-02-03T03:32:15Z" level=info msg="Stop pulling image manuonda/msvc-gateway:latest: Status: Downloaded newer image for manuonda/msvc-gateway:latest"
Feb 03 03:32:18 minikube cri-dockerd[1099]: time="2024-02-03T03:32:18Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-curso:latest: Status: Image is up to date for manuonda/curso-docker-msc-curso:latest"
Feb 03 03:32:20 minikube cri-dockerd[1099]: time="2024-02-03T03:32:20Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:32:23 minikube cri-dockerd[1099]: time="2024-02-03T03:32:23Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:32:25 minikube cri-dockerd[1099]: time="2024-02-03T03:32:25Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"
Feb 03 03:32:27 minikube cri-dockerd[1099]: time="2024-02-03T03:32:27Z" level=info msg="Stop pulling image manuonda/curso-docker-msc-usuarios:latest: Status: Image is up to date for manuonda/curso-docker-msc-usuarios:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                        CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
281d09108a284       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   9 minutes ago       Running             curso-docker-msc-usuarios   2                   0692eb9728c41       msvc-usuarios-6ff88998d-8lxfd
4ca9958a51ee4       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   9 minutes ago       Running             curso-docker-msc-usuarios   2                   2a4e9bb974c8f       msvc-usuarios-6ff88998d-mxhlg
a5fbda572eff0       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   9 minutes ago       Running             curso-docker-msc-usuarios   2                   dc035e2312511       msvc-usuarios-6ff88998d-bm7mp
518e06feb64d2       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   9 minutes ago       Running             curso-docker-msc-usuarios   2                   484c4482c2990       msvc-usuarios-6ff88998d-mxv7z
bf39fee7cf083       manuonda/curso-docker-msc-curso@sha256:a3a68b49b7adfeb4253bfad1d72ca63003cdc05e1c12b56a5de0a35be5fd6766      9 minutes ago       Running             curso-docker-msc-curso      11                  d166df4714cfd       msvc-cursos-5599cf6bbb-5n4r7
8eb4a64f515d5       manuonda/msvc-gateway@sha256:8c87a84e041a13c6e6ffd2085e78fd1cc6c25a22182ae374e4d4c48149463e0a                9 minutes ago       Running             msvc-gateway                0                   73a17f2728345       msvc-gateway-5c784c649d-dr9q9
cc25aba4c091f       postgres@sha256:4d1b17af6f66b852ee3a721f6691a2ca7352f9d28f570a6a48cee4ebe646b2fd                             9 minutes ago       Running             postgres                    8                   ae8465c8ec379       postgresql-container-598784c45b-pv6dl
da9dac47d4a08       6e38f40d628db                                                                                                9 minutes ago       Running             storage-provisioner         26                  590fc0b86a472       storage-provisioner
28c3bbb40df1b       07655ddf2eebe                                                                                                9 minutes ago       Running             kubernetes-dashboard        17                  9919c314a3b4a       kubernetes-dashboard-8694d4445c-hjsds
4ee9d58f002de       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   10 minutes ago      Exited              curso-docker-msc-usuarios   1                   2a4e9bb974c8f       msvc-usuarios-6ff88998d-mxhlg
9ef3f60e249ec       manuonda/curso-docker-msc-curso@sha256:a3a68b49b7adfeb4253bfad1d72ca63003cdc05e1c12b56a5de0a35be5fd6766      10 minutes ago      Exited              curso-docker-msc-curso      10                  d166df4714cfd       msvc-cursos-5599cf6bbb-5n4r7
4645ee48eb717       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   10 minutes ago      Exited              curso-docker-msc-usuarios   1                   0692eb9728c41       msvc-usuarios-6ff88998d-8lxfd
59ea22d1d048c       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   10 minutes ago      Exited              curso-docker-msc-usuarios   1                   484c4482c2990       msvc-usuarios-6ff88998d-mxv7z
197491e56e835       manuonda/curso-docker-msc-usuarios@sha256:0a8a8061f13e5affcf9346998d87ab1dfcabef59cf0994f2d0b834d2078348a8   10 minutes ago      Exited              curso-docker-msc-usuarios   1                   dc035e2312511       msvc-usuarios-6ff88998d-bm7mp
affa5748306cb       ead0a4a53df89                                                                                                10 minutes ago      Running             coredns                     17                  aa53e1902f1b9       coredns-5dd5756b68-2rks9
11ebc8a26a131       6e38f40d628db                                                                                                10 minutes ago      Exited              storage-provisioner         25                  590fc0b86a472       storage-provisioner
b699c2d6ea5b6       115053965e86b                                                                                                10 minutes ago      Running             dashboard-metrics-scraper   11                  40daf3875ef19       dashboard-metrics-scraper-7fd5cb4ddc-x6znq
c0ae130285d52       07655ddf2eebe                                                                                                10 minutes ago      Exited              kubernetes-dashboard        16                  9919c314a3b4a       kubernetes-dashboard-8694d4445c-hjsds
88b13be7cbef6       bfc896cf80fba                                                                                                10 minutes ago      Running             kube-proxy                  13                  509cecac3293d       kube-proxy-szgdk
db1d784115ab8       10baa1ca17068                                                                                                10 minutes ago      Running             kube-controller-manager     13                  e70ba1d957391       kube-controller-manager-minikube
aafd0976ad13c       6d1b4fd1b182d                                                                                                10 minutes ago      Running             kube-scheduler              13                  440cc924f64aa       kube-scheduler-minikube
ed85d215c81f9       73deb9a3f7025                                                                                                10 minutes ago      Running             etcd                        13                  c259350bdd4a4       etcd-minikube
323de0e865e72       5374347291230                                                                                                10 minutes ago      Running             kube-apiserver              13                  7ab846935ad19       kube-apiserver-minikube
4546399c78af1       postgres@sha256:49c276fa02e3d61bd9b8db81dfb4784fe814f50f778dce5980a03817438293e3                             2 days ago          Exited              postgres                    7                   a6b4629877f58       postgresql-container-598784c45b-pv6dl
048b82aaffa29       ead0a4a53df89                                                                                                2 days ago          Exited              coredns                     16                  162d9a0bf87cb       coredns-5dd5756b68-2rks9
db7abb00dffb3       115053965e86b                                                                                                2 days ago          Exited              dashboard-metrics-scraper   10                  e58b6c18e698e       dashboard-metrics-scraper-7fd5cb4ddc-x6znq
e002b8b7b1c74       bfc896cf80fba                                                                                                2 days ago          Exited              kube-proxy                  12                  5df4c2ef9decd       kube-proxy-szgdk
41136a3d7fca0       73deb9a3f7025                                                                                                2 days ago          Exited              etcd                        12                  5bf060e6deba0       etcd-minikube
37da32f987380       5374347291230                                                                                                2 days ago          Exited              kube-apiserver              12                  d588959f3adea       kube-apiserver-minikube
8acccab6325a1       6d1b4fd1b182d                                                                                                2 days ago          Exited              kube-scheduler              12                  37a888b8f010f       kube-scheduler-minikube
b7afdd27a98a9       10baa1ca17068                                                                                                2 days ago          Exited              kube-controller-manager     12                  ee5c8266fd1b7       kube-controller-manager-minikube

* 
* ==> coredns [048b82aaffa2] <==
* [INFO] 10.244.0.146:56656 - 51290 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000237993s
[INFO] 10.244.0.145:46710 - 8130 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000183686s
[INFO] 10.244.0.143:60578 - 23467 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000311695s
[INFO] 10.244.0.146:35637 - 16833 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000048388s
[INFO] 10.244.0.144:51310 - 4932 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000182495s
[INFO] 10.244.0.145:50830 - 42456 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000196634s
[INFO] 10.244.0.143:57681 - 1128 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000300263s
[INFO] 10.244.0.131:46230 - 58254 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000443344s
[INFO] 10.244.0.131:48337 - 54705 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000076655s
[INFO] 10.244.0.131:43766 - 13676 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000083857s
[INFO] 10.244.0.131:58318 - 30686 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000254828s
[INFO] 10.244.0.131:58942 - 23636 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000318876s
[INFO] 10.244.0.143:54364 - 52139 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000341787s
[INFO] 10.244.0.145:47398 - 3754 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000244836s
[INFO] 10.244.0.144:60236 - 6253 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000216622s
[INFO] 10.244.0.146:38544 - 32774 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000204178s
[INFO] 10.244.0.143:48527 - 48253 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000381982s
[INFO] 10.244.0.144:60556 - 19032 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000203043s
[INFO] 10.244.0.145:60373 - 27778 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000064718s
[INFO] 10.244.0.146:41030 - 64565 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000211512s
[INFO] 10.244.0.143:59099 - 42746 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000311214s
[INFO] 10.244.0.145:44243 - 21784 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.00017752s
[INFO] 10.244.0.131:48280 - 19529 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000304834s
[INFO] 10.244.0.131:37906 - 65248 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000316254s
[INFO] 10.244.0.131:52576 - 40875 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000086035s
[INFO] 10.244.0.131:58234 - 47293 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000084289s
[INFO] 10.244.0.131:57552 - 1046 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.00031611s
[INFO] 10.244.0.144:54486 - 4751 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000093443s
[INFO] 10.244.0.145:47081 - 44882 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.00018458s
[INFO] 10.244.0.146:57851 - 34007 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000211428s
[INFO] 10.244.0.143:54574 - 29858 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.00019203s
[INFO] 10.244.0.146:36396 - 5296 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000454057s
[INFO] 10.244.0.144:60831 - 59043 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000046301s
[INFO] 10.244.0.143:39119 - 12661 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000042638s
[INFO] 10.244.0.145:45976 - 21874 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000181539s
[INFO] 10.244.0.143:32966 - 60622 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000326035s
[INFO] 10.244.0.146:37742 - 53111 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000195019s
[INFO] 10.244.0.145:56518 - 30717 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000187949s
[INFO] 10.244.0.131:37918 - 4986 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000101803s
[INFO] 10.244.0.131:42207 - 61828 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000089942s
[INFO] 10.244.0.131:33687 - 43647 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000102118s
[INFO] 10.244.0.131:39000 - 36595 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.00008725s
[INFO] 10.244.0.131:47491 - 35330 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000081013s
[INFO] 10.244.0.144:37997 - 46216 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000107993s
[INFO] 10.244.0.143:44289 - 63310 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000048888s
[INFO] 10.244.0.145:39424 - 65278 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000130028s
[INFO] 10.244.0.146:39254 - 52150 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000067548s
[INFO] 10.244.0.145:37815 - 38295 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.00007961s
[INFO] 10.244.0.144:56409 - 15347 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000102057s
[INFO] 10.244.0.143:55807 - 56900 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000051421s
[INFO] 10.244.0.146:45871 - 28952 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000173799s
[INFO] 10.244.0.145:54132 - 4783 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000093593s
[INFO] 10.244.0.143:35232 - 54132 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000054051s
[INFO] 10.244.0.144:38989 - 26696 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000047625s
[INFO] 10.244.0.146:43596 - 33303 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000055538s
[INFO] 10.244.0.145:60353 - 25976 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000095402s
[INFO] 10.244.0.131:35205 - 10903 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000380651s
[INFO] 10.244.0.131:51379 - 8026 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000086208s
[INFO] 10.244.0.131:37898 - 38030 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000088798s
[INFO] 10.244.0.131:58185 - 61436 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000312372s

* 
* ==> coredns [affa5748306c] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:57373 - 17962 "HINFO IN 8435940261789592677.1765041113072320939. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.058374765s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.153:51421 - 11831 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000120287s
[INFO] 10.244.0.149:52273 - 63841 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000111565s
[INFO] 10.244.0.148:60468 - 3112 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000095321s
[INFO] 10.244.0.154:36568 - 12732 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000159313s
[INFO] 10.244.0.152:54573 - 50892 "A IN postgresql-container.default.svc.cluster.local. udp 64 false 512" NOERROR qr,aa,rd 126 0.000151519s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_12_16T10_11_19_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 16 Dec 2023 13:11:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 03 Feb 2024 03:41:46 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 03 Feb 2024 03:37:48 +0000   Sat, 16 Dec 2023 13:11:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 03 Feb 2024 03:37:48 +0000   Sat, 16 Dec 2023 13:11:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 03 Feb 2024 03:37:48 +0000   Sat, 16 Dec 2023 13:11:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 03 Feb 2024 03:37:48 +0000   Sat, 16 Dec 2023 13:11:16 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  143074460Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32618732Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  143074460Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32618732Ki
  pods:               110
System Info:
  Machine ID:                 904c9eb977a34bb6894ed0fa187d638c
  System UUID:                7d4ab8fb-7538-44e1-8f7d-3dd1fd7d8383
  Boot ID:                    bbd352cd-0ce4-4b3b-83d5-b8b4e62fa7ee
  Kernel Version:             6.5.0-15-generic
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     msvc-cursos-5599cf6bbb-5n4r7                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d2h
  default                     msvc-gateway-5c784c649d-dr9q9                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     msvc-usuarios-6ff88998d-8lxfd                 500m (4%!)(MISSING)     1 (8%!)(MISSING)      128Mi (0%!)(MISSING)       256Mi (0%!)(MISSING)     2d3h
  default                     msvc-usuarios-6ff88998d-bm7mp                 500m (4%!)(MISSING)     1 (8%!)(MISSING)      128Mi (0%!)(MISSING)       256Mi (0%!)(MISSING)     2d3h
  default                     msvc-usuarios-6ff88998d-mxhlg                 500m (4%!)(MISSING)     1 (8%!)(MISSING)      128Mi (0%!)(MISSING)       256Mi (0%!)(MISSING)     2d3h
  default                     msvc-usuarios-6ff88998d-mxv7z                 500m (4%!)(MISSING)     1 (8%!)(MISSING)      128Mi (0%!)(MISSING)       256Mi (0%!)(MISSING)     2d3h
  default                     postgresql-container-598784c45b-pv6dl         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d
  kube-system                 coredns-5dd5756b68-2rks9                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     48d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         48d
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         48d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         48d
  kube-system                 kube-proxy-szgdk                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         48d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         48d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         48d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-x6znq    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-hjsds         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                2750m (22%!)(MISSING)  4 (33%!)(MISSING)
  memory             682Mi (2%!)(MISSING)   1194Mi (3%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 10m                kube-proxy       
  Normal  Starting                 10m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  10m (x8 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    10m (x8 over 10m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     10m (x7 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  10m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           10m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +2.442222] evict_inodes inode 00000000c160d23f, i_count = 1, was skipped!
[  +0.000003] evict_inodes inode 000000001be3c50a, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 0000000038b740cc, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 000000003e4ff2b4, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000961eda9b, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000a690a3be, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000f7034830, i_count = 1, was skipped!
[  +0.028878] evict_inodes inode 00000000f8c86a97, i_count = 1, was skipped!
[  +0.000005] evict_inodes inode 00000000b593361b, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000756cc2a3, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000083b2ac84, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000019c7211f, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 000000001981d203, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000074b026b9, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000524efb42, i_count = 1, was skipped!
[  +2.142059] evict_inodes inode 0000000066ade7ec, i_count = 1, was skipped!
[  +0.000004] evict_inodes inode 00000000678afb86, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000c121a163, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000730cf3fc, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 0000000005c4df4f, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000063aafe68, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000e4a009fb, i_count = 1, was skipped!
[  +0.028103] evict_inodes inode 000000002ab469f6, i_count = 1, was skipped!
[  +0.000004] evict_inodes inode 000000000dca00ae, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 000000003b527ba1, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000062799b8d, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 000000005dcb7e36, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000aa22d246, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000e131bf00, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000b1af4fe1, i_count = 1, was skipped!
[  +1.880532] evict_inodes inode 000000009a3f434c, i_count = 1, was skipped!
[  +0.000004] evict_inodes inode 000000009a8eccc6, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000002f2b3c3, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 000000002dd4cec0, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000dc91eb6d, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000913a5d9e, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000880b0443, i_count = 1, was skipped!
[  +0.024209] evict_inodes inode 0000000005001426, i_count = 1, was skipped!
[  +0.000004] evict_inodes inode 000000000818d208, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000f967e247, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000bf192ec0, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 000000000153373a, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000a956f070, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 000000001afc2599, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 000000007afb4246, i_count = 1, was skipped!
[  +2.180886] evict_inodes inode 0000000011261dde, i_count = 1, was skipped!
[  +0.000004] evict_inodes inode 00000000ec330c6a, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000f26ce07f, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000d72b36ea, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 0000000094a312f0, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000c1519e87, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000b270506d, i_count = 1, was skipped!
[  +0.024037] evict_inodes inode 0000000017430257, i_count = 1, was skipped!
[  +0.000004] evict_inodes inode 00000000aef1ec8b, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000a956f070, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 000000000153373a, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 00000000bf192ec0, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 00000000f967e247, i_count = 1, was skipped!
[  +0.000001] evict_inodes inode 000000000818d208, i_count = 1, was skipped!
[  +0.000000] evict_inodes inode 0000000005001426, i_count = 1, was skipped!

* 
* ==> etcd [41136a3d7fca] <==
* {"level":"info","ts":"2024-02-01T01:16:52.437008Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":205182,"took":"464.896¬µs","hash":2449095827}
{"level":"info","ts":"2024-02-01T01:16:52.43702Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2449095827,"revision":205182,"compact-revision":204940}
{"level":"info","ts":"2024-02-01T01:21:52.442415Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":205425}
{"level":"info","ts":"2024-02-01T01:21:52.442813Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":205425,"took":"294.778¬µs","hash":1673913421}
{"level":"info","ts":"2024-02-01T01:21:52.442824Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1673913421,"revision":205425,"compact-revision":205182}
{"level":"info","ts":"2024-02-01T01:26:52.445017Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":205665}
{"level":"info","ts":"2024-02-01T01:26:52.445374Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":205665,"took":"272.224¬µs","hash":1430380684}
{"level":"info","ts":"2024-02-01T01:26:52.445386Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1430380684,"revision":205665,"compact-revision":205425}
{"level":"info","ts":"2024-02-01T01:31:52.449686Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":205906}
{"level":"info","ts":"2024-02-01T01:31:52.450034Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":205906,"took":"270.527¬µs","hash":184094951}
{"level":"info","ts":"2024-02-01T01:31:52.450044Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":184094951,"revision":205906,"compact-revision":205665}
{"level":"info","ts":"2024-02-01T01:36:52.453822Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":206146}
{"level":"info","ts":"2024-02-01T01:36:52.454188Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":206146,"took":"276.774¬µs","hash":1476720554}
{"level":"info","ts":"2024-02-01T01:36:52.454202Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1476720554,"revision":206146,"compact-revision":205906}
{"level":"info","ts":"2024-02-01T01:41:52.458518Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":206385}
{"level":"info","ts":"2024-02-01T01:41:52.458861Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":206385,"took":"266.02¬µs","hash":2569575021}
{"level":"info","ts":"2024-02-01T01:41:52.458873Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2569575021,"revision":206385,"compact-revision":206146}
{"level":"info","ts":"2024-02-01T01:46:52.463792Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":206627}
{"level":"info","ts":"2024-02-01T01:46:52.465295Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":206627,"took":"1.165434ms","hash":2544853137}
{"level":"info","ts":"2024-02-01T01:46:52.465306Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2544853137,"revision":206627,"compact-revision":206385}
{"level":"info","ts":"2024-02-01T01:51:52.46971Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":206866}
{"level":"info","ts":"2024-02-01T01:51:52.47009Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":206866,"took":"277.234¬µs","hash":3265815890}
{"level":"info","ts":"2024-02-01T01:51:52.470102Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3265815890,"revision":206866,"compact-revision":206627}
{"level":"info","ts":"2024-02-01T01:56:52.475241Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":207107}
{"level":"info","ts":"2024-02-01T01:56:52.476996Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":207107,"took":"1.403796ms","hash":3311094746}
{"level":"info","ts":"2024-02-01T01:56:52.47705Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3311094746,"revision":207107,"compact-revision":206866}
{"level":"info","ts":"2024-02-01T02:01:52.480617Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":207347}
{"level":"info","ts":"2024-02-01T02:01:52.480987Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":207347,"took":"289.705¬µs","hash":2374208742}
{"level":"info","ts":"2024-02-01T02:01:52.480997Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2374208742,"revision":207347,"compact-revision":207107}
{"level":"info","ts":"2024-02-01T02:06:52.484007Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":207588}
{"level":"info","ts":"2024-02-01T02:06:52.484352Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":207588,"took":"265.477¬µs","hash":463195591}
{"level":"info","ts":"2024-02-01T02:06:52.484364Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":463195591,"revision":207588,"compact-revision":207347}
{"level":"info","ts":"2024-02-01T02:11:52.486884Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":207828}
{"level":"info","ts":"2024-02-01T02:11:52.48726Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":207828,"took":"276.136¬µs","hash":1718764615}
{"level":"info","ts":"2024-02-01T02:11:52.487272Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1718764615,"revision":207828,"compact-revision":207588}
{"level":"info","ts":"2024-02-01T02:16:29.720844Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":260026,"local-member-snapshot-index":250025,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-02-01T02:16:29.722672Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":260026}
{"level":"info","ts":"2024-02-01T02:16:29.72273Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":255026}
{"level":"info","ts":"2024-02-01T02:16:52.129062Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/000000000000000d-0000000000033465.snap"}
{"level":"info","ts":"2024-02-01T02:16:52.488993Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":208068}
{"level":"info","ts":"2024-02-01T02:16:52.489434Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":208068,"took":"310.456¬µs","hash":1679332352}
{"level":"info","ts":"2024-02-01T02:16:52.489449Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1679332352,"revision":208068,"compact-revision":207828}
{"level":"info","ts":"2024-02-01T02:21:52.494037Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":208309}
{"level":"info","ts":"2024-02-01T02:21:52.494383Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":208309,"took":"265.134¬µs","hash":564266628}
{"level":"info","ts":"2024-02-01T02:21:52.494394Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":564266628,"revision":208309,"compact-revision":208068}
{"level":"info","ts":"2024-02-01T02:26:52.499102Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":208549}
{"level":"info","ts":"2024-02-01T02:26:52.500904Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":208549,"took":"1.453422ms","hash":10104}
{"level":"info","ts":"2024-02-01T02:26:52.500961Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":10104,"revision":208549,"compact-revision":208309}
{"level":"info","ts":"2024-02-01T02:31:52.503761Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":208789}
{"level":"info","ts":"2024-02-01T02:31:52.504106Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":208789,"took":"263.891¬µs","hash":152866698}
{"level":"info","ts":"2024-02-01T02:31:52.504118Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":152866698,"revision":208789,"compact-revision":208549}
{"level":"info","ts":"2024-02-01T02:36:52.50868Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":209030}
{"level":"info","ts":"2024-02-01T02:36:52.509029Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":209030,"took":"262.696¬µs","hash":899149368}
{"level":"info","ts":"2024-02-01T02:36:52.509039Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":899149368,"revision":209030,"compact-revision":208789}
{"level":"info","ts":"2024-02-01T02:41:52.513388Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":209270}
{"level":"info","ts":"2024-02-01T02:41:52.51381Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":209270,"took":"341.994¬µs","hash":1623778437}
{"level":"info","ts":"2024-02-01T02:41:52.513823Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1623778437,"revision":209270,"compact-revision":209030}
{"level":"info","ts":"2024-02-01T02:46:52.518118Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":209511}
{"level":"info","ts":"2024-02-01T02:46:52.518484Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":209511,"took":"278.851¬µs","hash":4037305758}
{"level":"info","ts":"2024-02-01T02:46:52.518495Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4037305758,"revision":209511,"compact-revision":209270}

* 
* ==> etcd [ed85d215c81f] <==
* {"level":"info","ts":"2024-02-03T03:31:09.853172Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 14"}
{"level":"info","ts":"2024-02-03T03:31:09.853178Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 14, commit: 261937, applied: 260026, lastindex: 261937, lastterm: 14]"}
{"level":"info","ts":"2024-02-03T03:31:09.853233Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-03T03:31:09.853444Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-03T03:31:09.853452Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-03T03:31:09.853855Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-03T03:31:09.854254Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":209511}
{"level":"info","ts":"2024-02-03T03:31:09.855459Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":209814}
{"level":"info","ts":"2024-02-03T03:31:09.856108Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-03T03:31:09.856897Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-03T03:31:09.857026Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-03T03:31:09.857044Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-03T03:31:09.857065Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-03T03:31:09.857278Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-03T03:31:09.857304Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-03T03:31:09.857311Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-03T03:31:09.858168Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-03T03:31:09.858215Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-03T03:31:09.858225Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-03T03:31:09.858268Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-03T03:31:09.858288Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-03T03:31:10.154396Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 14"}
{"level":"info","ts":"2024-02-03T03:31:10.154558Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 14"}
{"level":"info","ts":"2024-02-03T03:31:10.154614Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 14"}
{"level":"info","ts":"2024-02-03T03:31:10.154642Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 15"}
{"level":"info","ts":"2024-02-03T03:31:10.15466Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 15"}
{"level":"info","ts":"2024-02-03T03:31:10.154684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 15"}
{"level":"info","ts":"2024-02-03T03:31:10.154705Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 15"}
{"level":"info","ts":"2024-02-03T03:31:10.156443Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-03T03:31:10.15649Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-03T03:31:10.156493Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-03T03:31:10.157021Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-03T03:31:10.157069Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-03T03:31:10.159302Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-03T03:31:10.159697Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"warn","ts":"2024-02-03T03:32:01.96206Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128026926874075230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-02-03T03:32:02.462495Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128026926874075230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-02-03T03:32:02.96362Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128026926874075230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-02-03T03:32:03.461325Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.000032324s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context deadline exceeded"}
{"level":"info","ts":"2024-02-03T03:32:03.46152Z","caller":"traceutil/trace.go:171","msg":"trace[1892307587] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"2.000250509s","start":"2024-02-03T03:32:01.461216Z","end":"2024-02-03T03:32:03.461466Z","steps":["trace[1892307587] 'agreement among raft nodes before linearized reading'  (duration: 2.000021708s)"],"step_count":1}
{"level":"warn","ts":"2024-02-03T03:32:03.461604Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-02-03T03:32:01.461184Z","time spent":"2.000394206s","remote":"127.0.0.1:44648","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-02-03T03:32:03.464476Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128026926874075230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-02-03T03:32:03.585123Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"2.348530868s","expected-duration":"1s"}
{"level":"info","ts":"2024-02-03T03:32:03.585306Z","caller":"traceutil/trace.go:171","msg":"trace[400643911] transaction","detail":"{read_only:false; response_revision:210054; number_of_response:1; }","duration":"2.348893769s","start":"2024-02-03T03:32:01.236402Z","end":"2024-02-03T03:32:03.585295Z","steps":["trace[400643911] 'process raft request'  (duration: 2.348815706s)"],"step_count":1}
{"level":"warn","ts":"2024-02-03T03:32:03.585566Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-02-03T03:32:01.236374Z","time spent":"2.348951658s","remote":"127.0.0.1:44914","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:210036 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-02-03T03:32:03.621652Z","caller":"traceutil/trace.go:171","msg":"trace[1680404590] linearizableReadLoop","detail":"{readStateIndex:262203; appliedIndex:262201; }","duration":"2.160313417s","start":"2024-02-03T03:32:01.461273Z","end":"2024-02-03T03:32:03.621586Z","steps":["trace[1680404590] 'read index received'  (duration: 2.12394812s)","trace[1680404590] 'applied index is now lower than readState.Index'  (duration: 36.364982ms)"],"step_count":2}
{"level":"info","ts":"2024-02-03T03:32:03.621654Z","caller":"traceutil/trace.go:171","msg":"trace[904623800] transaction","detail":"{read_only:false; response_revision:210055; number_of_response:1; }","duration":"2.075679756s","start":"2024-02-03T03:32:01.545964Z","end":"2024-02-03T03:32:03.621644Z","steps":["trace[904623800] 'process raft request'  (duration: 2.075573594s)"],"step_count":1}
{"level":"warn","ts":"2024-02-03T03:32:03.621783Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.836640253s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:134"}
{"level":"warn","ts":"2024-02-03T03:32:03.621784Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-02-03T03:32:01.545935Z","time spent":"2.075781573s","remote":"127.0.0.1:44914","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:210037 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-02-03T03:32:03.621784Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.190621ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-02-03T03:32:03.621804Z","caller":"traceutil/trace.go:171","msg":"trace[743097608] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:210055; }","duration":"1.836662498s","start":"2024-02-03T03:32:01.785136Z","end":"2024-02-03T03:32:03.621799Z","steps":["trace[743097608] 'agreement among raft nodes before linearized reading'  (duration: 1.836618531s)"],"step_count":1}
{"level":"info","ts":"2024-02-03T03:32:03.621809Z","caller":"traceutil/trace.go:171","msg":"trace[1350779704] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:210055; }","duration":"159.254905ms","start":"2024-02-03T03:32:03.462548Z","end":"2024-02-03T03:32:03.621803Z","steps":["trace[1350779704] 'agreement among raft nodes before linearized reading'  (duration: 159.210673ms)"],"step_count":1}
{"level":"warn","ts":"2024-02-03T03:32:03.621818Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-02-03T03:32:01.785112Z","time spent":"1.83670278s","remote":"127.0.0.1:44684","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":158,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"info","ts":"2024-02-03T03:32:03.787427Z","caller":"traceutil/trace.go:171","msg":"trace[112545154] transaction","detail":"{read_only:false; response_revision:210056; number_of_response:1; }","duration":"163.503078ms","start":"2024-02-03T03:32:03.623916Z","end":"2024-02-03T03:32:03.787419Z","steps":["trace[112545154] 'process raft request'  (duration: 163.436679ms)"],"step_count":1}
{"level":"info","ts":"2024-02-03T03:32:03.787427Z","caller":"traceutil/trace.go:171","msg":"trace[199709101] linearizableReadLoop","detail":"{readStateIndex:262205; appliedIndex:262203; }","duration":"161.954543ms","start":"2024-02-03T03:32:03.62546Z","end":"2024-02-03T03:32:03.787415Z","steps":["trace[199709101] 'read index received'  (duration: 158.427137ms)","trace[199709101] 'applied index is now lower than readState.Index'  (duration: 3.527061ms)"],"step_count":2}
{"level":"warn","ts":"2024-02-03T03:32:03.787494Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.035157ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-02-03T03:32:03.787511Z","caller":"traceutil/trace.go:171","msg":"trace[755065498] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:210056; }","duration":"162.061991ms","start":"2024-02-03T03:32:03.625444Z","end":"2024-02-03T03:32:03.787506Z","steps":["trace[755065498] 'agreement among raft nodes before linearized reading'  (duration: 162.009145ms)"],"step_count":1}
{"level":"info","ts":"2024-02-03T03:41:10.180683Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":210313}
{"level":"info","ts":"2024-02-03T03:41:10.193266Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":210313,"took":"12.43412ms","hash":957012760}
{"level":"info","ts":"2024-02-03T03:41:10.193289Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":957012760,"revision":210313,"compact-revision":209511}

* 
* ==> kernel <==
*  03:41:48 up 17:25,  0 users,  load average: 1.81, 1.49, 1.30
Linux minikube 6.5.0-15-generic #15~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [323de0e865e7] <==
* I0203 03:31:10.657606       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0203 03:31:10.657621       1 controller.go:78] Starting OpenAPI AggregationController
I0203 03:31:10.657628       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0203 03:31:10.657632       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0203 03:31:10.657689       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0203 03:31:10.657722       1 controller.go:116] Starting legacy_token_tracking_controller
I0203 03:31:10.657709       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0203 03:31:10.657729       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0203 03:31:10.657733       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0203 03:31:10.657736       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0203 03:31:10.657770       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0203 03:31:10.657781       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0203 03:31:10.657785       1 controller.go:134] Starting OpenAPI controller
I0203 03:31:10.657797       1 controller.go:85] Starting OpenAPI V3 controller
I0203 03:31:10.657803       1 naming_controller.go:291] Starting NamingConditionController
I0203 03:31:10.657726       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0203 03:31:10.658010       1 establishing_controller.go:76] Starting EstablishingController
I0203 03:31:10.658041       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0203 03:31:10.658067       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0203 03:31:10.658076       1 crd_finalizer.go:266] Starting CRDFinalizer
I0203 03:31:10.658118       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0203 03:31:10.658179       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0203 03:31:10.658186       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0203 03:31:10.658230       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0203 03:31:10.673277       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0203 03:31:10.684040       1 shared_informer.go:318] Caches are synced for node_authorizer
I0203 03:31:10.758540       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0203 03:31:10.758655       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0203 03:31:10.758693       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0203 03:31:10.758739       1 aggregator.go:166] initial CRD sync complete...
I0203 03:31:10.758761       1 autoregister_controller.go:141] Starting autoregister controller
I0203 03:31:10.758777       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0203 03:31:10.758799       1 cache.go:39] Caches are synced for autoregister controller
I0203 03:31:10.758923       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0203 03:31:10.759016       1 shared_informer.go:318] Caches are synced for configmaps
I0203 03:31:10.759107       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0203 03:31:10.759153       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
E0203 03:31:10.776419       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0203 03:31:11.661088       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0203 03:31:12.268553       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0203 03:31:12.274218       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0203 03:31:12.286486       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0203 03:31:12.294288       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0203 03:31:12.296907       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0203 03:31:22.796644       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0203 03:31:22.800134       1 controller.go:624] quota admission added evaluator for: endpoints
I0203 03:31:38.817802       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0203 03:32:03.585793       1 trace.go:236] Trace[687461510]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2e105d16-3aa0-44b5-a73e-ea9cf52d6fb3,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (03-Feb-2024 03:32:01.234) (total time: 2351ms):
Trace[687461510]: ["GuaranteedUpdate etcd3" audit-id:2e105d16-3aa0-44b5-a73e-ea9cf52d6fb3,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 2351ms (03:32:01.234)
Trace[687461510]:  ---"Txn call completed" 2349ms (03:32:03.585)]
Trace[687461510]: [2.35125774s] [2.35125774s] END
I0203 03:32:03.622086       1 trace.go:236] Trace[1972350218]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4f15653f-3b92-4317-8928-0a7702952701,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (03-Feb-2024 03:32:01.544) (total time: 2077ms):
Trace[1972350218]: ["GuaranteedUpdate etcd3" audit-id:4f15653f-3b92-4317-8928-0a7702952701,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 2077ms (03:32:01.544)
Trace[1972350218]:  ---"Txn call completed" 2076ms (03:32:03.621)]
Trace[1972350218]: [2.077933143s] [2.077933143s] END
I0203 03:32:03.792092       1 trace.go:236] Trace[1687073512]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (03-Feb-2024 03:32:01.784) (total time: 2007ms):
Trace[1687073512]: ---"initial value restored" 1837ms (03:32:03.621)
Trace[1687073512]: ---"Transaction prepared" 162ms (03:32:03.784)
Trace[1687073512]: [2.007494289s] [2.007494289s] END
I0203 03:33:08.537674       1 alloc.go:330] "allocated clusterIPs" service="default/msvc-gateway" clusterIPs={"IPv4":"10.96.250.209"}

* 
* ==> kube-apiserver [37da32f98738] <==
* I0131 12:56:52.263579       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0131 12:56:52.263627       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0131 12:56:52.263693       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0131 12:56:52.263629       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0131 12:56:52.263699       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0131 12:56:52.263796       1 controller.go:85] Starting OpenAPI V3 controller
I0131 12:56:52.263851       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0131 12:56:52.263867       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0131 12:56:52.263878       1 crd_finalizer.go:266] Starting CRDFinalizer
I0131 12:56:52.263883       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0131 12:56:52.263889       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0131 12:56:52.263890       1 available_controller.go:423] Starting AvailableConditionController
I0131 12:56:52.263904       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0131 12:56:52.263650       1 controller.go:116] Starting legacy_token_tracking_controller
I0131 12:56:52.263915       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0131 12:56:52.263646       1 controller.go:78] Starting OpenAPI AggregationController
I0131 12:56:52.263935       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0131 12:56:52.263941       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0131 12:56:52.263711       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0131 12:56:52.263722       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0131 12:56:52.263718       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0131 12:56:52.264068       1 naming_controller.go:291] Starting NamingConditionController
I0131 12:56:52.263633       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0131 12:56:52.263760       1 aggregator.go:164] waiting for initial CRD sync...
I0131 12:56:52.263784       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0131 12:56:52.263785       1 controller.go:134] Starting OpenAPI controller
I0131 12:56:52.263648       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0131 12:56:52.263811       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0131 12:56:52.263750       1 establishing_controller.go:76] Starting EstablishingController
I0131 12:56:52.301763       1 shared_informer.go:318] Caches are synced for node_authorizer
I0131 12:56:52.364444       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0131 12:56:52.364463       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0131 12:56:52.364470       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0131 12:56:52.364491       1 aggregator.go:166] initial CRD sync complete...
I0131 12:56:52.364498       1 autoregister_controller.go:141] Starting autoregister controller
I0131 12:56:52.364502       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0131 12:56:52.364508       1 cache.go:39] Caches are synced for autoregister controller
I0131 12:56:52.364451       1 shared_informer.go:318] Caches are synced for configmaps
I0131 12:56:52.364451       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0131 12:56:52.364594       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0131 12:56:52.364600       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0131 12:56:52.376202       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0131 12:56:53.265350       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0131 12:56:53.888448       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0131 12:56:53.892437       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0131 12:56:53.906156       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0131 12:56:53.915827       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0131 12:56:53.918693       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0131 12:57:04.494888       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0131 12:57:04.510037       1 controller.go:624] quota admission added evaluator for: endpoints
I0131 13:10:00.721490       1 alloc.go:330] "allocated clusterIPs" service="default/msvc-usuarios" clusterIPs={"IPv4":"10.106.125.18"}
I0131 19:22:14.599253       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0131 22:30:19.156325       1 trace.go:236] Trace[295236555]: "Update" accept:application/json, */*,audit-id:59058dbd-f179-4e7c-81b8-32f515b3ff2c,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Jan-2024 22:30:18.540) (total time: 616ms):
Trace[295236555]: ["GuaranteedUpdate etcd3" audit-id:59058dbd-f179-4e7c-81b8-32f515b3ff2c,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 616ms (22:30:18.540)
Trace[295236555]:  ---"Txn call completed" 615ms (22:30:19.156)]
Trace[295236555]: [616.236393ms] [616.236393ms] END
I0131 23:57:22.588228       1 trace.go:236] Trace[1418716970]: "Update" accept:application/json, */*,audit-id:adfe9c84-6995-484d-b611-41260d49b794,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Jan-2024 23:57:21.225) (total time: 1362ms):
Trace[1418716970]: ["GuaranteedUpdate etcd3" audit-id:adfe9c84-6995-484d-b611-41260d49b794,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1362ms (23:57:21.225)
Trace[1418716970]:  ---"Txn call completed" 1362ms (23:57:22.587)]
Trace[1418716970]: [1.362640311s] [1.362640311s] END

* 
* ==> kube-controller-manager [b7afdd27a98a] <==
* I0131 23:52:52.361757       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="28.683¬µs"
I0131 23:52:53.371178       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="3.540344ms"
I0131 23:52:53.371232       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="29.546¬µs"
I0131 23:53:04.095088       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="38.278¬µs"
I0131 23:55:37.376288       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="3.615039ms"
I0131 23:55:37.376348       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="30.058¬µs"
I0131 23:55:38.385725       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="2.496294ms"
I0131 23:55:38.385789       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="40.275¬µs"
I0131 23:55:43.487261       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="3.214¬µs"
I0201 00:00:23.043488       1 event.go:307] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-74c5956459 to 1"
I0201 00:00:23.061487       1 event.go:307] "Event occurred" object="default/msvc-usuarios-74c5956459" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-74c5956459-8hph8"
I0201 00:00:23.071638       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="28.172223ms"
I0201 00:00:23.081735       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="10.065143ms"
I0201 00:00:23.081776       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="22.655¬µs"
I0201 00:00:23.090229       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="57.815¬µs"
I0201 00:00:40.160213       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="7.100431ms"
I0201 00:00:40.160251       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="20.104¬µs"
I0201 00:07:32.167435       1 event.go:307] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-59f85d85f7 to 1"
I0201 00:07:32.171040       1 event.go:307] "Event occurred" object="default/msvc-usuarios-59f85d85f7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-59f85d85f7-tm96t"
I0201 00:07:32.173742       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="6.351721ms"
I0201 00:07:32.178473       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="4.6946ms"
I0201 00:07:32.178548       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="36.03¬µs"
I0201 00:07:32.180455       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="60.273¬µs"
I0201 00:07:34.730670       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="43.126¬µs"
I0201 00:07:52.628427       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="2.927693ms"
I0201 00:07:52.628508       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="57.464¬µs"
I0201 00:07:52.632288       1 event.go:307] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set msvc-usuarios-74c5956459 to 0 from 1"
I0201 00:07:52.635041       1 event.go:307] "Event occurred" object="default/msvc-usuarios-74c5956459" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: msvc-usuarios-74c5956459-8hph8"
I0201 00:07:52.639050       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="6.972413ms"
I0201 00:07:52.642804       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="3.710225ms"
I0201 00:07:52.642856       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="28.896¬µs"
I0201 00:07:52.905060       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="54.543¬µs"
I0201 00:07:53.896286       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="29.256¬µs"
I0201 00:07:53.897841       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="56.093¬µs"
I0201 00:14:59.389438       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-59f85d85f7" duration="4.307¬µs"
I0201 00:14:59.389453       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-74c5956459" duration="11.385¬µs"
I0201 00:15:11.247336       1 event.go:307] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-6ff88998d to 4"
I0201 00:15:11.250309       1 event.go:307] "Event occurred" object="default/msvc-usuarios-6ff88998d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-6ff88998d-8lxfd"
I0201 00:15:11.252706       1 event.go:307] "Event occurred" object="default/msvc-usuarios-6ff88998d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-6ff88998d-mxv7z"
I0201 00:15:11.252727       1 event.go:307] "Event occurred" object="default/msvc-usuarios-6ff88998d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-6ff88998d-mxhlg"
I0201 00:15:11.256087       1 event.go:307] "Event occurred" object="default/msvc-usuarios-6ff88998d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-6ff88998d-bm7mp"
I0201 00:15:11.260293       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="13.042476ms"
I0201 00:15:11.264154       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="3.814951ms"
I0201 00:15:11.264229       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="47.249¬µs"
I0201 00:15:11.264254       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="14.54¬µs"
I0201 00:15:11.267696       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="51.643¬µs"
I0201 00:15:11.271337       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="34.807¬µs"
I0201 00:15:11.274887       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="43.752¬µs"
I0201 00:15:14.591129       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="88.471¬µs"
I0201 00:15:15.604837       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="66.422¬µs"
I0201 00:15:17.624776       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="51.145¬µs"
I0201 00:15:20.647766       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="48.872¬µs"
I0201 00:15:31.821048       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="4.295133ms"
I0201 00:15:31.821127       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="43.512¬µs"
I0201 00:15:31.827767       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="3.017075ms"
I0201 00:15:31.827812       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="25.405¬µs"
I0201 00:15:31.876621       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="3.12965ms"
I0201 00:15:31.876750       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="79.568¬µs"
I0201 00:15:51.822732       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="4.208492ms"
I0201 00:15:51.822783       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="27.762¬µs"

* 
* ==> kube-controller-manager [db1d784115ab] <==
* I0203 03:31:22.963901       1 shared_informer.go:318] Caches are synced for resource quota
I0203 03:31:22.984381       1 shared_informer.go:318] Caches are synced for persistent volume
I0203 03:31:22.984417       1 shared_informer.go:318] Caches are synced for attach detach
I0203 03:31:23.040759       1 shared_informer.go:318] Caches are synced for PV protection
I0203 03:31:23.102088       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="164.829205ms"
I0203 03:31:23.102192       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="58.422¬µs"
I0203 03:31:23.102854       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgresql-container-598784c45b" duration="165.744408ms"
I0203 03:31:23.102892       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="165.840977ms"
I0203 03:31:23.102894       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="165.817268ms"
I0203 03:31:23.103004       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgresql-container-598784c45b" duration="50.404¬µs"
I0203 03:31:23.103152       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="26.782¬µs"
I0203 03:31:23.103175       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="30.913¬µs"
I0203 03:31:23.377101       1 shared_informer.go:318] Caches are synced for garbage collector
I0203 03:31:23.380285       1 shared_informer.go:318] Caches are synced for garbage collector
I0203 03:31:23.380311       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0203 03:31:23.559640       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="3.960779ms"
I0203 03:31:23.559897       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="87.7¬µs"
I0203 03:31:25.575231       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="51.4¬µs"
I0203 03:31:31.622271       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="55.336¬µs"
I0203 03:31:32.629806       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="53.484¬µs"
I0203 03:31:33.646726       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="2.926493ms"
I0203 03:31:33.646777       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="30.401¬µs"
I0203 03:31:34.398939       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="31.956¬µs"
I0203 03:31:34.656000       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="48.603¬µs"
I0203 03:31:38.692040       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="35.511¬µs"
I0203 03:31:38.819674       1 event.go:307] "Event occurred" object="default/msvc-gateway" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-gateway-5c784c649d to 1"
I0203 03:31:38.822252       1 event.go:307] "Event occurred" object="default/msvc-gateway-5c784c649d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-gateway-5c784c649d-dr9q9"
I0203 03:31:38.824695       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-gateway-5c784c649d" duration="5.120002ms"
I0203 03:31:38.826716       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-gateway-5c784c649d" duration="1.988916ms"
I0203 03:31:38.826778       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-gateway-5c784c649d" duration="35.978¬µs"
I0203 03:31:38.831509       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-gateway-5c784c649d" duration="33.579¬µs"
I0203 03:31:40.258361       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="34.157¬µs"
I0203 03:31:40.589261       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="32.041¬µs"
I0203 03:31:43.698436       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="65.569¬µs"
I0203 03:31:44.741480       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="4.45753ms"
I0203 03:31:44.741569       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="54.339¬µs"
I0203 03:31:46.012903       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="50.354¬µs"
I0203 03:31:46.593123       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="3.571726ms"
I0203 03:31:46.593189       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="42.197¬µs"
I0203 03:31:54.024386       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="33.733¬µs"
I0203 03:31:54.854761       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="33.331834ms"
I0203 03:31:54.854800       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="22.088¬µs"
I0203 03:32:04.956655       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgresql-container-598784c45b" duration="11.768729ms"
I0203 03:32:04.956692       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgresql-container-598784c45b" duration="21.396¬µs"
I0203 03:32:16.050707       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-gateway-5c784c649d" duration="8.018303ms"
I0203 03:32:16.050780       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-gateway-5c784c649d" duration="40.162¬µs"
I0203 03:32:19.075168       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="7.321638ms"
I0203 03:32:19.075213       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-cursos-5599cf6bbb" duration="27.917¬µs"
I0203 03:32:22.176524       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="69.301¬µs"
I0203 03:32:24.199378       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="79.784¬µs"
I0203 03:32:26.229119       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="106.623¬µs"
I0203 03:32:28.260660       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="65.695¬µs"
I0203 03:32:34.827681       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="12.212019ms"
I0203 03:32:34.828286       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="87.246¬µs"
I0203 03:32:40.792396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="9.21231ms"
I0203 03:32:40.792456       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="40.753¬µs"
I0203 03:32:53.938204       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="7.52433ms"
I0203 03:32:53.938260       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="37.602¬µs"
I0203 03:32:54.987233       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="8.882ms"
I0203 03:32:54.987273       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/msvc-usuarios-6ff88998d" duration="23.664¬µs"

* 
* ==> kube-proxy [88b13be7cbef] <==
* I0203 03:31:13.668966       1 server_others.go:69] "Using iptables proxy"
I0203 03:31:13.681868       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0203 03:31:13.700853       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0203 03:31:13.701861       1 server_others.go:152] "Using iptables Proxier"
I0203 03:31:13.701873       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0203 03:31:13.701876       1 server_others.go:438] "Defaulting to no-op detect-local"
I0203 03:31:13.702418       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0203 03:31:13.702641       1 server.go:846] "Version info" version="v1.28.3"
I0203 03:31:13.702648       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0203 03:31:13.703310       1 config.go:97] "Starting endpoint slice config controller"
I0203 03:31:13.703316       1 config.go:188] "Starting service config controller"
I0203 03:31:13.703580       1 config.go:315] "Starting node config controller"
I0203 03:31:13.704171       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0203 03:31:13.704176       1 shared_informer.go:311] Waiting for caches to sync for node config
I0203 03:31:13.704174       1 shared_informer.go:311] Waiting for caches to sync for service config
I0203 03:31:13.804851       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0203 03:31:13.804927       1 shared_informer.go:318] Caches are synced for node config
I0203 03:31:13.804992       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [e002b8b7b1c7] <==
* I0131 12:56:53.727010       1 server_others.go:69] "Using iptables proxy"
I0131 12:56:53.732097       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0131 12:56:53.740654       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0131 12:56:53.741710       1 server_others.go:152] "Using iptables Proxier"
I0131 12:56:53.741721       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0131 12:56:53.741725       1 server_others.go:438] "Defaulting to no-op detect-local"
I0131 12:56:53.741736       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0131 12:56:53.741831       1 server.go:846] "Version info" version="v1.28.3"
I0131 12:56:53.741836       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0131 12:56:53.742059       1 config.go:97] "Starting endpoint slice config controller"
I0131 12:56:53.742070       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0131 12:56:53.742098       1 config.go:315] "Starting node config controller"
I0131 12:56:53.742099       1 config.go:188] "Starting service config controller"
I0131 12:56:53.742107       1 shared_informer.go:311] Waiting for caches to sync for node config
I0131 12:56:53.742120       1 shared_informer.go:311] Waiting for caches to sync for service config
I0131 12:56:53.842389       1 shared_informer.go:318] Caches are synced for service config
I0131 12:56:53.842397       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0131 12:56:53.842399       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [8acccab6325a] <==
* I0131 12:56:50.911873       1 serving.go:348] Generated self-signed cert in-memory
W0131 12:56:52.273803       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0131 12:56:52.273877       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0131 12:56:52.273901       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0131 12:56:52.273916       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0131 12:56:52.321047       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0131 12:56:52.321059       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0131 12:56:52.321662       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0131 12:56:52.321677       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0131 12:56:52.322074       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0131 12:56:52.322100       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0131 12:56:52.422001       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [aafd0976ad13] <==
* I0203 03:31:09.345250       1 serving.go:348] Generated self-signed cert in-memory
I0203 03:31:10.701575       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0203 03:31:10.701586       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0203 03:31:10.703530       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0203 03:31:10.703530       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0203 03:31:10.703651       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0203 03:31:10.703867       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0203 03:31:10.703894       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0203 03:31:10.703930       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0203 03:31:10.703930       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0203 03:31:10.703933       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0203 03:31:10.804138       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0203 03:31:10.804185       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0203 03:31:10.804322       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file

* 
* ==> kubelet <==
* Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.991360    1564 apiserver.go:52] "Watching apiserver"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993326    1564 topology_manager.go:215] "Topology Admit Handler" podUID="7fa3e48c-c7ff-4ca0-b8df-20261283ea0f" podNamespace="kube-system" podName="storage-provisioner"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993386    1564 topology_manager.go:215] "Topology Admit Handler" podUID="dd35841d-0afa-45a5-bee3-008439993346" podNamespace="kube-system" podName="kube-proxy-szgdk"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993452    1564 topology_manager.go:215] "Topology Admit Handler" podUID="7deed438-53ae-47cb-97fb-d38ea3b2dded" podNamespace="kube-system" podName="coredns-5dd5756b68-2rks9"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993512    1564 topology_manager.go:215] "Topology Admit Handler" podUID="d89978e9-2d77-46bb-b128-ecf7d279db58" podNamespace="kubernetes-dashboard" podName="kubernetes-dashboard-8694d4445c-hjsds"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993681    1564 topology_manager.go:215] "Topology Admit Handler" podUID="9a21323e-3fae-4f81-b973-c3f80aa7bf25" podNamespace="kubernetes-dashboard" podName="dashboard-metrics-scraper-7fd5cb4ddc-x6znq"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993754    1564 topology_manager.go:215] "Topology Admit Handler" podUID="93b629fd-f64c-44e1-8011-73344ea0c335" podNamespace="default" podName="postgresql-container-598784c45b-pv6dl"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993789    1564 topology_manager.go:215] "Topology Admit Handler" podUID="b4dfae84-29c6-4a83-b875-3a9c93a15d2b" podNamespace="default" podName="msvc-cursos-5599cf6bbb-5n4r7"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993832    1564 topology_manager.go:215] "Topology Admit Handler" podUID="1fb2d1a2-6947-408a-a2c6-399d10f05a62" podNamespace="default" podName="msvc-usuarios-6ff88998d-bm7mp"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993936    1564 topology_manager.go:215] "Topology Admit Handler" podUID="7fc0d13d-e702-44b2-8318-d7bb399bc577" podNamespace="default" podName="msvc-usuarios-6ff88998d-mxv7z"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.993997    1564 topology_manager.go:215] "Topology Admit Handler" podUID="81e42fba-bab6-4b87-a7f8-88572103cf4a" podNamespace="default" podName="msvc-usuarios-6ff88998d-8lxfd"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.994036    1564 topology_manager.go:215] "Topology Admit Handler" podUID="5d5966a3-53ba-4307-a2be-560c93daf88f" podNamespace="default" podName="msvc-usuarios-6ff88998d-mxhlg"
Feb 03 03:31:12 minikube kubelet[1564]: I0203 03:31:12.999516    1564 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Feb 03 03:31:13 minikube kubelet[1564]: I0203 03:31:13.085558    1564 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"postgresql-data-pv\" (UniqueName: \"kubernetes.io/host-path/93b629fd-f64c-44e1-8011-73344ea0c335-postgresql-data-pv\") pod \"postgresql-container-598784c45b-pv6dl\" (UID: \"93b629fd-f64c-44e1-8011-73344ea0c335\") " pod="default/postgresql-container-598784c45b-pv6dl"
Feb 03 03:31:13 minikube kubelet[1564]: I0203 03:31:13.085584    1564 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/dd35841d-0afa-45a5-bee3-008439993346-lib-modules\") pod \"kube-proxy-szgdk\" (UID: \"dd35841d-0afa-45a5-bee3-008439993346\") " pod="kube-system/kube-proxy-szgdk"
Feb 03 03:31:13 minikube kubelet[1564]: I0203 03:31:13.085607    1564 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/dd35841d-0afa-45a5-bee3-008439993346-xtables-lock\") pod \"kube-proxy-szgdk\" (UID: \"dd35841d-0afa-45a5-bee3-008439993346\") " pod="kube-system/kube-proxy-szgdk"
Feb 03 03:31:13 minikube kubelet[1564]: I0203 03:31:13.085761    1564 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/7fa3e48c-c7ff-4ca0-b8df-20261283ea0f-tmp\") pod \"storage-provisioner\" (UID: \"7fa3e48c-c7ff-4ca0-b8df-20261283ea0f\") " pod="kube-system/storage-provisioner"
Feb 03 03:31:13 minikube kubelet[1564]: W0203 03:31:13.133769    1564 helpers.go:242] readString: Failed to read "/sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod93b629fd_f64c_44e1_8011_73344ea0c335.slice/cpuset.cpus.effective": read /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod93b629fd_f64c_44e1_8011_73344ea0c335.slice/cpuset.cpus.effective: no such device
Feb 03 03:31:13 minikube kubelet[1564]: I0203 03:31:13.344984    1564 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9919c314a3b4a847f3f325c7b150698b5e8f86f6c59fe7dd232e5675fe50456a"
Feb 03 03:31:16 minikube kubelet[1564]: I0203 03:31:16.584475    1564 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 03 03:31:17 minikube kubelet[1564]: I0203 03:31:17.426141    1564 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 03 03:31:31 minikube kubelet[1564]: I0203 03:31:31.610771    1564 scope.go:117] "RemoveContainer" containerID="2253092defa1f2f7a5a79f7ef245734bad74583e6cc4286763eb0406a570f073"
Feb 03 03:31:31 minikube kubelet[1564]: I0203 03:31:31.610985    1564 scope.go:117] "RemoveContainer" containerID="197491e56e835a580acb094a31eb29087d8637a2fe7780cb4ee7504190a16054"
Feb 03 03:31:31 minikube kubelet[1564]: E0203 03:31:31.611179    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-bm7mp_default(1fb2d1a2-6947-408a-a2c6-399d10f05a62)\"" pod="default/msvc-usuarios-6ff88998d-bm7mp" podUID="1fb2d1a2-6947-408a-a2c6-399d10f05a62"
Feb 03 03:31:32 minikube kubelet[1564]: I0203 03:31:32.624721    1564 scope.go:117] "RemoveContainer" containerID="235a69563f71917e10dc80afcf7300b37751d521f3403905efa98515a970a975"
Feb 03 03:31:32 minikube kubelet[1564]: I0203 03:31:32.624954    1564 scope.go:117] "RemoveContainer" containerID="59ea22d1d048c20f75badc67aa386aa9149bd4ab9d44b5e21e064d2bcf0dbfc0"
Feb 03 03:31:32 minikube kubelet[1564]: E0203 03:31:32.625143    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-mxv7z_default(7fc0d13d-e702-44b2-8318-d7bb399bc577)\"" pod="default/msvc-usuarios-6ff88998d-mxv7z" podUID="7fc0d13d-e702-44b2-8318-d7bb399bc577"
Feb 03 03:31:33 minikube kubelet[1564]: I0203 03:31:33.638802    1564 scope.go:117] "RemoveContainer" containerID="69e3d68577319ecd3829ff63ac00e4b34d27dc7d4fab0fa4f8864eb365896bdf"
Feb 03 03:31:33 minikube kubelet[1564]: I0203 03:31:33.638991    1564 scope.go:117] "RemoveContainer" containerID="9ef3f60e249ec3b8b57c6f7ebd0c8eb92b9479c0db7919db92d293a4166568be"
Feb 03 03:31:33 minikube kubelet[1564]: E0203 03:31:33.639126    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-curso\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-curso pod=msvc-cursos-5599cf6bbb-5n4r7_default(b4dfae84-29c6-4a83-b875-3a9c93a15d2b)\"" pod="default/msvc-cursos-5599cf6bbb-5n4r7" podUID="b4dfae84-29c6-4a83-b875-3a9c93a15d2b"
Feb 03 03:31:34 minikube kubelet[1564]: I0203 03:31:34.393734    1564 scope.go:117] "RemoveContainer" containerID="59ea22d1d048c20f75badc67aa386aa9149bd4ab9d44b5e21e064d2bcf0dbfc0"
Feb 03 03:31:34 minikube kubelet[1564]: E0203 03:31:34.393905    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-mxv7z_default(7fc0d13d-e702-44b2-8318-d7bb399bc577)\"" pod="default/msvc-usuarios-6ff88998d-mxv7z" podUID="7fc0d13d-e702-44b2-8318-d7bb399bc577"
Feb 03 03:31:34 minikube kubelet[1564]: I0203 03:31:34.648778    1564 scope.go:117] "RemoveContainer" containerID="f969d7de6ad14389d005fabc1e8ad0544e0816b952602926fa4769ba1473107f"
Feb 03 03:31:34 minikube kubelet[1564]: I0203 03:31:34.648998    1564 scope.go:117] "RemoveContainer" containerID="4645ee48eb717ccf9cf0074bc61d6e440df6301bc26cf769417193b7fd918d2e"
Feb 03 03:31:34 minikube kubelet[1564]: E0203 03:31:34.649205    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-8lxfd_default(81e42fba-bab6-4b87-a7f8-88572103cf4a)\"" pod="default/msvc-usuarios-6ff88998d-8lxfd" podUID="81e42fba-bab6-4b87-a7f8-88572103cf4a"
Feb 03 03:31:38 minikube kubelet[1564]: I0203 03:31:38.686666    1564 scope.go:117] "RemoveContainer" containerID="3e8de66bcf1ab54697a2b72c734e8905e9622707d328975d5474be13ddc891e6"
Feb 03 03:31:38 minikube kubelet[1564]: I0203 03:31:38.686887    1564 scope.go:117] "RemoveContainer" containerID="4ee9d58f002deaa80f00efea87f065a037b4a8ce4b4b3767dfb4401392b34daf"
Feb 03 03:31:38 minikube kubelet[1564]: E0203 03:31:38.687068    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-mxhlg_default(5d5966a3-53ba-4307-a2be-560c93daf88f)\"" pod="default/msvc-usuarios-6ff88998d-mxhlg" podUID="5d5966a3-53ba-4307-a2be-560c93daf88f"
Feb 03 03:31:38 minikube kubelet[1564]: I0203 03:31:38.825907    1564 topology_manager.go:215] "Topology Admit Handler" podUID="00d33786-c0f7-447b-ae6e-0da059ef099d" podNamespace="default" podName="msvc-gateway-5c784c649d-dr9q9"
Feb 03 03:31:38 minikube kubelet[1564]: I0203 03:31:38.917752    1564 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9dtq9\" (UniqueName: \"kubernetes.io/projected/00d33786-c0f7-447b-ae6e-0da059ef099d-kube-api-access-9dtq9\") pod \"msvc-gateway-5c784c649d-dr9q9\" (UID: \"00d33786-c0f7-447b-ae6e-0da059ef099d\") " pod="default/msvc-gateway-5c784c649d-dr9q9"
Feb 03 03:31:40 minikube kubelet[1564]: I0203 03:31:40.253033    1564 scope.go:117] "RemoveContainer" containerID="197491e56e835a580acb094a31eb29087d8637a2fe7780cb4ee7504190a16054"
Feb 03 03:31:40 minikube kubelet[1564]: E0203 03:31:40.253275    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-bm7mp_default(1fb2d1a2-6947-408a-a2c6-399d10f05a62)\"" pod="default/msvc-usuarios-6ff88998d-bm7mp" podUID="1fb2d1a2-6947-408a-a2c6-399d10f05a62"
Feb 03 03:31:40 minikube kubelet[1564]: I0203 03:31:40.581487    1564 scope.go:117] "RemoveContainer" containerID="4ee9d58f002deaa80f00efea87f065a037b4a8ce4b4b3767dfb4401392b34daf"
Feb 03 03:31:40 minikube kubelet[1564]: E0203 03:31:40.582236    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-mxhlg_default(5d5966a3-53ba-4307-a2be-560c93daf88f)\"" pod="default/msvc-usuarios-6ff88998d-mxhlg" podUID="5d5966a3-53ba-4307-a2be-560c93daf88f"
Feb 03 03:31:43 minikube kubelet[1564]: I0203 03:31:43.668506    1564 scope.go:117] "RemoveContainer" containerID="4645ee48eb717ccf9cf0074bc61d6e440df6301bc26cf769417193b7fd918d2e"
Feb 03 03:31:43 minikube kubelet[1564]: E0203 03:31:43.668724    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"curso-docker-msc-usuarios\" with CrashLoopBackOff: \"back-off 10s restarting failed container=curso-docker-msc-usuarios pod=msvc-usuarios-6ff88998d-8lxfd_default(81e42fba-bab6-4b87-a7f8-88572103cf4a)\"" pod="default/msvc-usuarios-6ff88998d-8lxfd" podUID="81e42fba-bab6-4b87-a7f8-88572103cf4a"
Feb 03 03:31:44 minikube kubelet[1564]: I0203 03:31:44.731781    1564 scope.go:117] "RemoveContainer" containerID="372c5c1fb395ccf160a775cd945e2e29bebdbddd3af515d540fc077052d5d505"
Feb 03 03:31:44 minikube kubelet[1564]: I0203 03:31:44.731955    1564 scope.go:117] "RemoveContainer" containerID="c0ae130285d52751528251dbfc016fc9c6967a6471cfca32b8f12782983f6a6e"
Feb 03 03:31:44 minikube kubelet[1564]: E0203 03:31:44.732099    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-hjsds_kubernetes-dashboard(d89978e9-2d77-46bb-b128-ecf7d279db58)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-hjsds" podUID="d89978e9-2d77-46bb-b128-ecf7d279db58"
Feb 03 03:31:44 minikube kubelet[1564]: I0203 03:31:44.735386    1564 scope.go:117] "RemoveContainer" containerID="11ebc8a26a1314f98cf341c0d63b167ee0fa14f7c48aea86cfbffd99189d7838"
Feb 03 03:31:44 minikube kubelet[1564]: E0203 03:31:44.735499    1564 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(7fa3e48c-c7ff-4ca0-b8df-20261283ea0f)\"" pod="kube-system/storage-provisioner" podUID="7fa3e48c-c7ff-4ca0-b8df-20261283ea0f"
Feb 03 03:31:44 minikube kubelet[1564]: I0203 03:31:44.739170    1564 scope.go:117] "RemoveContainer" containerID="fb59432f78c66ee8e22918ddce5c01c0aeb3cc70fdfacab52fc0ff7b9fec734b"
Feb 03 03:31:46 minikube kubelet[1564]: I0203 03:31:46.008392    1564 scope.go:117] "RemoveContainer" containerID="9ef3f60e249ec3b8b57c6f7ebd0c8eb92b9479c0db7919db92d293a4166568be"
Feb 03 03:31:47 minikube kubelet[1564]: I0203 03:31:47.008486    1564 scope.go:117] "RemoveContainer" containerID="59ea22d1d048c20f75badc67aa386aa9149bd4ab9d44b5e21e064d2bcf0dbfc0"
Feb 03 03:31:54 minikube kubelet[1564]: I0203 03:31:54.019811    1564 scope.go:117] "RemoveContainer" containerID="c0ae130285d52751528251dbfc016fc9c6967a6471cfca32b8f12782983f6a6e"
Feb 03 03:31:55 minikube kubelet[1564]: I0203 03:31:55.007777    1564 scope.go:117] "RemoveContainer" containerID="197491e56e835a580acb094a31eb29087d8637a2fe7780cb4ee7504190a16054"
Feb 03 03:31:56 minikube kubelet[1564]: I0203 03:31:56.007930    1564 scope.go:117] "RemoveContainer" containerID="4ee9d58f002deaa80f00efea87f065a037b4a8ce4b4b3767dfb4401392b34daf"
Feb 03 03:31:57 minikube kubelet[1564]: I0203 03:31:57.007973    1564 scope.go:117] "RemoveContainer" containerID="4645ee48eb717ccf9cf0074bc61d6e440df6301bc26cf769417193b7fd918d2e"
Feb 03 03:32:00 minikube kubelet[1564]: I0203 03:32:00.008313    1564 scope.go:117] "RemoveContainer" containerID="11ebc8a26a1314f98cf341c0d63b167ee0fa14f7c48aea86cfbffd99189d7838"
Feb 03 03:32:16 minikube kubelet[1564]: I0203 03:32:16.042910    1564 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/msvc-gateway-5c784c649d-dr9q9" podStartSLOduration=2.148651763 podCreationTimestamp="2024-02-03 03:31:38 +0000 UTC" firstStartedPulling="2024-02-03 03:31:39.262640713 +0000 UTC m=+31.418937860" lastFinishedPulling="2024-02-03 03:32:15.156803405 +0000 UTC m=+67.313100554" observedRunningTime="2024-02-03 03:32:16.042644047 +0000 UTC m=+68.198941196" watchObservedRunningTime="2024-02-03 03:32:16.042814457 +0000 UTC m=+68.199111603"

* 
* ==> kubernetes-dashboard [28c3bbb40df1] <==
* 2024/02/03 03:31:54 Starting overwatch
2024/02/03 03:31:54 Using namespace: kubernetes-dashboard
2024/02/03 03:31:54 Using in-cluster config to connect to apiserver
2024/02/03 03:31:54 Using secret token for csrf signing
2024/02/03 03:31:54 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/02/03 03:31:54 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/02/03 03:31:54 Successful initial request to the apiserver, version: v1.28.3
2024/02/03 03:31:54 Generating JWE encryption key
2024/02/03 03:31:54 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/02/03 03:31:54 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/03 03:31:54 Initializing JWE encryption key from synchronized object
2024/02/03 03:31:54 Creating in-cluster Sidecar client
2024/02/03 03:31:54 Successful request to sidecar
2024/02/03 03:31:54 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [c0ae130285d5] <==
* 2024/02/03 03:31:13 Using namespace: kubernetes-dashboard
2024/02/03 03:31:13 Using in-cluster config to connect to apiserver
2024/02/03 03:31:13 Using secret token for csrf signing
2024/02/03 03:31:13 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/02/03 03:31:13 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc0008dfae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00047c680)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [11ebc8a26a13] <==
* I0203 03:31:13.677123       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0203 03:31:43.683556       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [da9dac47d4a0] <==
* I0203 03:32:03.914460       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0203 03:32:03.933482       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0203 03:32:03.933714       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0203 03:32:21.332132       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0203 03:32:21.332230       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8e4f1238-21c4-47bc-8aa1-44ebb54d85cf!
I0203 03:32:21.332229       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"17464549-316f-4818-9250-5d641681e947", APIVersion:"v1", ResourceVersion:"210088", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8e4f1238-21c4-47bc-8aa1-44ebb54d85cf became leader
I0203 03:32:21.433003       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8e4f1238-21c4-47bc-8aa1-44ebb54d85cf!

